<chapter title="How to visualize decision tree models"
         author={[Terence Parr](http://parrt.cs.usfca.edu) and [Prince Grover](https://www.linkedin.com/in/groverpr)}>


Decision trees are the fundamental building block of [gradient boosting machines](http://explained.ai/gradient-boosting/index.html) and [Random Forests](https://en.wikipedia.org/wiki/Random_forest)\symbol{tm}, probably the two most popular machine learning models for structured data.  Visualizing decision trees is a tremendous aid when learning these models and later, in practice, when interpreting models.  Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. We also couldn't find a package that visualized a specific $\vec{x}$ feature vector being run down the decision tree to the leaf providing the target $\vec{y}$.

Terence has reached a point in his book on [machine learning book](https://mlbook.explained.ai/) (written with [Jeremy Howard](http://www.fast.ai/about/#jeremy)) where he needs to describe lots of decision trees. So, together with former  [University of San Francisco MS in Data Science](https://www.usfca.edu/arts-sciences/graduate-programs/data-science) student Prince Grover, we have created a general package for [scikit-learn](https://github.com/scikit-learn/scikit-learn) decision tree visualization and model interpretation. This article demonstrates the results of this work, details the specific choices we made for visualization, and outlines the wild mashup of tools and techniques used in the implementation.

<section title="Gallery of decision tree visualizations">
	
[full gallery](https://github.com/parrt/animl/tree/master/testing/samples)

<table>
	<tr>
		<td>[<img src="images/icons/wine-TD-2-icon.png" width="100%">](images/wine-TD-2.png)
		<td>[<img src="images/icons/breast_cancer-LR-3-icon.png" width="100%">](images/breast_cancer-LR-3.png)	<tr>
		<td>[<img src="images/icons/iris-TD-3-X-icon.png" width="100%">](images/iris-TD-3-X.png)
		<td>[<img src="images/icons/knowledge-LR-3-icon.png" width="100%">](images/knowledge-LR-3.png)
	<tr>
		<td>[<img src="images/icons/digits-LR-3-icon.png" width="100%">](images/digits-LR-3.png)
		<td>[<img src="images/icons/diabetes-TD-3-X-icon.png" width="100%">](images/diabetes-TD-3-X.png)	
	<tr>
		<td>[<img src="images/icons/boston-TD-3-X-icon.png" width="100%">](images/boston-TD-3-X.png)
		<td>[<img src="images/icons/sweets-TD-3-X-icon.png" width="100%">](images/sweets-TD-3-X.png)
			<tr>
				<td>[<img src="images/icons/knowledge-TD-4-simple-icon.png" width="100%">](images/knowledge-TD-4-simple.png)
				<td>[<img src="images/icons/diabetes-TD-4-simple-icon.png" width="100%">](images/diabetes-TD-4-simple.png)



</table>	

Sample code.


<section title="Current state-of-the-art visualizations">
	
If you do a web search for "visualizing decision trees" you will quickly find a visualization provided by the awesome scikit folks themselves: [sklearn.tree.export_graphviz](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html).  For example, here is [their visualization](http://scikit-learn.org/stable/modules/tree.html) of a decision tree on the [IRIS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) data set (click on image to enlarge).

[<img src="images/iris-scikit.png" width="60%">](images/iris.png)

This image does a good job of representing the tree structure, but we have a few quibbles.  The colors aren't the best and it's not immediately obvious why some of the nodes are colored and some aren't.  If the colors represent predicted class for this classifier, one would think just the leaves would be colored because only leaves have predictions.  Including the gini coefficient costs space and doesn't really help the beginners. The count of samples of the  various target classes in each node is very useful, though, a histogram would be even better. A target class color legend would be nice.  Finally, using true and false as the edge labels isn't as clear as, say, labels \< and >=.

The same mechanism works for decision tree regressors. For example, here is a visualization using the [BOSTON](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) data set (click to enlarge image):

[<img src="images/boston-scikit.png" width="70%">](images/boston-scikit.png)

In this image, it's not immediately clear what the use of color implies, but after studying the image, darker images indicate higher predicted target values.

If you dig really hard, you will find that SAS and IBM provide (non-Python-based) decision tree visualizations.  For example, the visualization from SAS includes some interesting details:
	
[<img src="images/sas-tree.png" width="40%">](images/sas-tree.png)

The histogram at each node is a nice touch.  Here's an example visualization from [IBM's Watson analytics](https://www.ibm.com/support/knowledgecenter/en/SS4QC9/com.ibm.solutions.wa_an_overview.2.0.0.doc/wa_discover_viz_expl_insigths_dec_tree.html) that appears to be on the [TITANIC](https://www.kaggle.com/c/titanic/data) data set:

[<img src="images/ibm-tree.png" width="50%">](images/ibm-tree.png)

The explicit labels for equality or inequality improve understanding and, like the SAS tree, the legend is also helpful.

All of these visualizations are useful, but we weren't truly inspired until we saw the impressive [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/), which shows an (animated) decision tree like this:

[<img src="images/vizml-tree.png" width="75%">](images/vizml-tree.png)

Unfortunately, that visualization is hardcoded to a specific data set and is not a general library. It has two unique characteristics, though, aside from the animation: the leaf size is proportional to the number of samples in that leaf and the split point for decision nodes is shown visually in the histogram/distribution.

<section title="Our decision tree visualizations">

decision trees carve up the feature space, looking for groups of observations (feature vectors) together that have similar (regression) or the same (classification) target values. Predictions for unknown observations are derived from the
	
While the implementation is virtually identical for both classifier and regressor decision trees, the way we interpret them is very different so our visualizations are clearly different for the two cases.

What's important?  Variable name and split point for decision nodes and  prediction, purity, and number of samples for leaves.

	
<subsection title="Classifier decision trees">
	


	started with simple
	
	[<img src="images/iris-TD-4-simple.png" width="55%">](images/iris-TD-4-simple.png)

IRIS:

[<img src="images/iris-TD-5.png" width="75%">](images/iris-TD-5.png)

[USER KNOWLEDGE](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)
	
[<img src="images/knowledge-TD-4.png" width="95%">](images/knowledge-TD-4.png)

when there are too many classes and starts to break down and so we set the limit at 10.
 [DIGITS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)

[<img src="images/digits-TD-2.png" width="65%">](images/digits-TD-2.png)

<subsection title="Regressor decision trees">

What's important? leaves:  prediction, purity, sample size as before but we need a new visualization other than pie chart. For decision nodes, we need variable name and split point.

	started with simple
	
	[<img src="images/boston-TD-4-simple.png" width="95%">](images/boston-TD-4-simple.png)

	BOSTON

	[<img src="images/boston-TD-3.png" width="75%">](images/boston-TD-3.png)

<section title="Visualizing tree interpreter for single instance">

	KNOWLEDGE
	
	[<img src="images/class-demo.png" width="75%">](images/class-demo.png)

	[DIABETES](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)

	[<img src="images/diabetes-TD-3-X.png" width="75%">](images/diabetes-TD-3-X.png)

<section title="Left-to-right layout">
	
	
	<section title="Code sample">
		
```
regr = tree.DecisionTreeRegressor(max_depth=max_depth)
boston = load_boston()
regr = regr.fit(boston.data, boston.target)
```

```
st = dtreeviz(regr, boston.data, boston.target, target_name='price',
              feature_names=boston.feature_names)
```

```
g = graphviz.Source(st, format='pdf')
g.render(directory=".", filename="boston.pdf", view=False, cleanup=True)
```

```
g = graphviz.Source(st, format='pdf')
g.view()
```

```
X = boston.data[np.random.randint(0, len(boston.data)),:]

st = dtreeviz(regr, boston.data, boston.target, target_name='price',
              feature_names=boston.feature_names,
              X=X)
```

<section title="Our implementation">
	
	shadow tree
	
	graphviz/dot

	matplotlib

	load svg was issue

	then must parse svg to get size

	future: bottom-justify histograms in classifier trees. some wedge labels overlap with axis. thicken incoming edges for nodes with lots of samples.
	
<section title="What we tried and rejected">

classifier decision nodes

	<img src="images/kde.png" width="50%">
	
	<img src="images/bubble.png" width="70%">

regression leaf nodes		

	<img src="images/dual-leaf.png" width="15%">
		
	<img src="images/non-strip-plot.png" width="30%">