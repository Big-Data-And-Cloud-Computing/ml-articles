<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118361649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-118361649-1');
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
<title>Explaining RNNs without neural networks</title>
<!-- META -->
<!-- LinkedIn meta -->
<meta property='og:title' content="Explaining RNNs without neural networks"/>
<meta property='og:image' content="http://explained.ai/rnn/images/">
<meta property='og:description' content=""/>
<meta property='og:url' content="http://explained.ai/rnn/index.html"/>

<!-- Facebook meta -->
<meta property="og:type" content="article" />

<!-- Twitter meta -->
<meta name="twitter:title" content="Explaining RNNs without neural networks">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@the_antlr_guy">
<meta name="twitter:creator" content="@the_antlr_guy">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="http://explained.ai/rnn/images/">
<!-- END META -->
</head>
<body>
<div class="watermark">
<i>Brought to you by <a href='http://explained.ai'>explained.ai</a></i><br>
</div>

<h1>Explaining RNNs without neural networks</h1>

<p><a href="http://parrt.cs.usfca.edu">Terence Parr</a><br><span style="font-size: 85%">Terence teaches in <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">University of San Francisco's MS in Data Science program</a> and you might know him as the creator of the ANTLR parser generator.</span></p>

<p>
Vanilla recurrent neural networks (RNNs) form the basis of more sophisticated models, such as LSTMs and GRUs. There are lots of great articles, books, and videos that describe the functionality, mathematics, and behavior of RNNs, so I'm not trying to add yet another rehash here. (See below for a list of resources.) My goal is to present an explanation that avoids the neural network metaphor, stripping it down to its essence&mdash;a series of vector transformations that result in embeddings for variable-length input vectors.
</p>

<p>
My learning style involves pounding away at something until I'm able to re-create it myself from fundamental components. This helps me to understand exactly <b>what</b> a model is doing and <b>why</b> it is doing it.  You can ignore this article if you're familiar with  standard neural network layers and are comfortable with RNN explanations that use them as building blocks. Since I'm still learning the details of neural networks, I wanted to (1) peer through those layers to the matrices and vectors beneath and (2) investigate the details of the training process. My starting point was <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">Karpathy's RNN code snippet</a> associated with <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness">The Unreasonable Effectiveness of Recurrent Neural Networks</a> and then I absorbed minibatch details from Chapter 12 from Jeremy Howard's / Sylvan Gugger's book, <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">Deep Learning for Coders with fastai and PyTorch</a> and Chapter 12 from Andrew Trask's <a href="https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709">Grokking Deep Learning</a>.
</p>

<p>
In this article, I hope to contribute a simple and visually-focused data-transformation perspective on RNNs, complete with PyTorch implementations  that learn to predict the natural language associated with family names using:
</p>

<ul>
	<li><a href="notebooks/SGD.ipynb">SGD</a> (parameters updated after each record)
	<li><a href="notebooks/gpu.ipynb">minibatch</a> (parameters updated after a small batch of records)
	<li><a href="notebooks/minibatch.ipynb">vectorized minibatch</a> (convert for-loop into matrix multiply)
	<li><a href="notebooks/vectorized.ipynb">vectorized minibatch running on a GPU</a> (use PyTorch to compute on GPU)
</ul>

notebooks/prep.ipynb

lessons learned: h is not part of the model. when do we update parameters?  when do we reset h? what is h?  Is V part of the RNN? relu  explodes unless we constrain  model parameters. use tanh

<h2>TL;DR Details learned </h2>

<p></p>

For those readers who already have a good understanding of RNN concepts, let me summarize some of the key implementation bits I learned while coding these examples:

<ul>
	<li>minibatching between records and within records
	<li>`h` is initialized to zero for each word.
	<li>`h` is not updated as part of the gradient descent process; `h` is a temporary variable holding partial results, not part of the model.
	<li> must pad on the right not the left
	<li> bptt have anything to do with this? no
	<li>Matrices $W$, $U$, $V$ are initialized exactly once before training begins.
	<li>$W$, $U$, $V$ are updated as part of the gradient descent process but only once per word. That is, they are updated after the  dense vector `h` has been computed for each word.
	<li>As we iterate through the characters of a single word, the $W$, $U$, and $V$ matrices do not change.
</ul>

<h2>Table of contents</h2>

<a href="implementation.html"> implementation</a><br><br>
<a href="minibatch.html"> minibatch</a><br><br>
<a href="gpu.html">GPU</a><br><br>

<h2>Acknowledgements</h2>

<h2>Resources</h2>

To get started, I recommend <a href="https://www.youtube.com/watch?v=SEnXr6v2ifU">MIT's RNN intro video</a>

https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb

http://karpathy.github.io/2015/05/21/rnn-effectiveness/

chap 6 in   deep learning with Python book; does some low-level stuff. well-written focuses on keras

Chap 12 Andrew W. Trask. <a href="https://www.amazon.com/Grokking-Deep-Learning-Andrew-Trask/dp/1617293709">Grokking Deep Learning</a> has a lot of good stuff on word vectors and RNNs.

chap 11 and 12 from grokking; tries to get at the intuition. 12 is more about language models.

Yannet Interian  notebook https://github.com/yanneta/dl-course/blob/master/rnn-name2lang.ipynb

</body>
</html>
