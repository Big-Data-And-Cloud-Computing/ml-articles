<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />
<link rel="stylesheet" type="text/css" href="css/article.css"/>
</head>
<body>
<h1>How to explain gradient boosting</h1>

<p id="author">
<p><a href="http://parrt.cs.usfca.edu">Terence Parr</a> and <a href="http://www.fast.ai/about/#jeremy">Jeremy Howard</a></p>
</p>

<p><font size=-1>(We teach in University of San Francisco's <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">MS in Data Science program</a> and have other nefarious projects underway. You might know Terence as the creator of the <a href="http://www.antlr.org">ANTLR parser generator</a>. Jeremy is a founding researcher at <a href="http://fast.ai">fast.ai</a>, a research institute dedicated to making deep learning more accessible.)</font></p>
</p>

<div id="toc">
<p class="toc_title">Contents</p>
<ul>
	<li><a href="#overview">Roadmap</a>
	<li><a href="L2-loss.html">Distance to target</a>
		<ul>
			<li><a href="L2-loss.html#sec1">An introduction to boosted regression</a></li>
			<ul>
			</ul>
			<li><a href="L2-loss.html#sec2">The intuition behind gradient boosting</a></li>
			<ul>
			</ul>
			<li><a href="L2-loss.html#sec3">Gradient boosting regression by example</a></li>
			<li><a href="#sec5">Measuring model performance</a></li>
			<li><a href="#sec4">Choosing hyper-parameters</a></li>
		</ul>
	<li><a href="L1-loss.html">Heading in the right direction</a>
	<li><a href="descent.html#sec3.1">Gradient boosting performs gradient descent</a>
</ul>
</div>

<h3 id="overview">Roadmap</h3>

<p><img style="float:left;margin:0px 10px 0px 0;" width=180 src="images/golf-dir-vector.png"><a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient boosting machines</a> (GBMs) are currently very popular and so it's a good idea for machine learning practitioners to understand how GBMs work. The problem is that understanding all of the mathematical machinery is tricky and, unfortunately, these details are needed to tune the hyper-parameters. (Tuning the hyper-parameters is required to get a decent GBM model unlike, say, Random Forests.) Our goal in this article is to explain the intuition behind gradient boosting, provide visualizations for model construction, explain the mathematics as simply as possible, and answer thorny questions such as why GBM is performing &ldquo;gradient descent in function space.&rdquo; We've split the discussion into three morsels for easier digestion.
</p>

<ol>

<li><a href="L2-loss.html">Gradient boosting: Distance to target</a>. <br>
First, we examine the most common form of GBM that optimizes the mean squared error (MSE), also called the <i>L</i><sub>2</sub> <i>loss</i> or <i>cost</i>. (The mean squared error is the average of the square of the difference between the true targets and the predicted values from a set of observations, such as a training or validation set.)  As we'll see, A GBM is a composite model that combines the efforts of multiple weak models to create a strong model, and each additional weak model reduces the mean squared error (MSE) of the overall model.  We give a fully-worked GBM example for a simple data set, complete with computations and model visualizations.<br><br>

<li><a href="L1-loss.html">Gradient boosting: Heading in the right direction</a>.<br>
Optimizing a model according to MSE makes it chase outliers because squaring the difference between targets and predicted values emphasizes extreme values. When we can't remove outliers, it's better to optimize the mean absolute error (MAE), also called the <i>L</i><sub>1</sub> <i>loss</i> or <i>cost</i>. (The MAE is the average of the absolute value of the difference between the true targets and the predicted values.) This second article shows the computations and visualizations for a GBM that optimizes MAE for the same data set as used in the first article to optimize MSE.<br><br>

<li><a href="descent.html">Gradient boosting performs gradient descent</a>.<br>
The previous two articles give the intuition behind GBM and the simple formulas to show how weak models join forces to create a strong regression model.  No attempt was made to prove that these models converge on a useful prediction or even that model construction terminates. This last article demonstrates that gradient boosting is really doing a form of gradient descent and, therefore, is in fact optimizing MSE or MAE depending on the residuals (direction vectors) we use to train the weak models. The discussion relies on a bit of derivative calculus but it's an important read if you'd like to learn deeply how GBM works. (To brush up on your vectors and derivatives, you can check out <a href="http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html">The Matrix Calculus You Need For Deep Learning</a>.) We finish off with a translation of our notation to that of Friedman in his original paper on gradient boosting.

<p>As Ben Gorman points out in <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">A Kaggle Master Explains Gradient Boosting</a>, &ldquo;<i>This is the part that gets butchered by a lot of gradient boosting explanations</i>.&rdquo; His blog post does a good job of explaining it, but we give our own perspective here.  

</ol>

<h3>Recommended reading</h3>

<p>To get started with GBM, those with very strong mathematical backgrounds can go directly to the super-tasty 1999 paper by Jerome Friedman called <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine</a>.  To get the practical and implementation perspective, though, we recommend Ben Gorman's excellent blog <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">A Kaggle Master Explains Gradient Boosting</a> and Prince Grover's <a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">Gradient Boosting from scratch</a> (written by a <a href="https://www.usfca.edu/arts-sciences/graduate-programs/data-science">data science</a> graduate student at the University of San Francisco). To go beyond basic GBM, see  <a href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost: A Scalable Tree Boosting System</a> by Chen and Guestrin.</p>


</body>
</html>