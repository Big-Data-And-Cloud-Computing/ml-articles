<chapter title="How to visualize decision tree models"
         author={[Terence Parr](http://parrt.cs.usfca.edu) and [Prince Grover](https://www.linkedin.com/in/groverpr)}>


Decision trees are the fundamental building block of [gradient boosting machines](http://explained.ai/gradient-boosting/index.html) and [Random Forests](https://en.wikipedia.org/wiki/Random_forest)\symbol{tm}, probably the two most popular machine learning models for structured data.  Visualizing decision trees is a tremendous aid when learning these models and later, in practice, when interpreting models.  Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. We also couldn't find a package that visualized a specific $\vec{x}$ feature vector being run down the decision tree to the leaf providing the target $\vec{y}$.

Terence has reached a point in his book on [machine learning book](https://mlbook.explained.ai/) (written with [Jeremy Howard](http://www.fast.ai/about/#jeremy)) where he needs to describe lots of decision trees. So, he and former [University of San Francisco MS in Data Science](https://www.usfca.edu/arts-sciences/graduate-programs/data-science) student Prince Grover teamed up to create a general package for [scikit-learn](https://github.com/scikit-learn/scikit-learn) decision tree visualization and model interpretation. This article demonstrates the results of this work, details the specific choices we made for visualization, and outlines the mashup of tools and techniques used in the implementation. The visualization software is part of a nascent Python machine learning library called [animl](https://github.com/parrt/animl).

We assume you're familiar with the basic mechanism of decision trees if you're interested in visualizing them, but let's start with a brief summary so that we're all using the same terminology.

<section title="Decision tree review">

A decision tree is a machine learning model based upon binary trees, trees with at most a left and right child.  A decision tree learns the relationship between feature vectors $\vec{x}$ and target values $y$ (in a training set) by examining and condensing training data into a binary tree of interior decision nodes and leaf prediction nodes.  

Each leaf in the decision tree is responsible for making a specific prediction.  If this is a regression tree, the prediction is a value in the target space, such as price.  If this is a classifier tree, the prediction is an integer indicating the predicted target class, such as cancer or not-cancer. A decision tree carves up the feature space into groups of feature vectors that share similar target values and each leaf represents one of these groups.  For regression, "similar" means a low variance between target values and, for classification, "similar" means that most or all targets are of a single class.

Any path from the root of the decision tree to a specific $y$ leaf predictor passes through a series of (internal) decision nodes. Each decision node compares a single feature's value in $\vec{x}$, $x_i$, with a specific \first{split point} value learned during training. For example, in a model predicting apartment rent price, decision nodes would test features such as the number of bedrooms and number of bathrooms.  In a classifier with discrete target values, decision node still compare numeric *feature* values. Most decision tree implementation's assume that all features are numeric, with all categorical variables [one hot encoded](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/) or [binned](https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/) or [label encoded](http://forums.fast.ai/t/to-label-encode-or-one-hot-encode/6057).

To train a decision node, the model examines a subset of the training observations (or the full training set at the root). The node's feature and split point within that feature space are chosen during training to split the observations into left and right buckets (subsets). The left bucket has observations whose $x_i$ feature values are all less than the split point and the right bucket has observations whose $x_i$ is greater than the split point.  The goal is to pick a feature and split point in that feature space so that, within the left and right buckets, the observations have similar target values. (This selection process is generally done through exhaustive comparison of features and  feature values.) Tree construction proceeds recursively by creating decision nodes for the left bucket and the right bucket.  Given a bucket with suitably similar target values, the model creates a leaf node rather than a decision node. The leaf predicts the mean (regression) or most common target (classification).

<section title="The key elements of decision tree visualization">
	
Decision tree visualizations would optimally highlight the following important elements:

<ul>
	<li>Decision node feature vs target value distributions
	<li>Decision node feature name
	<li>Decision node feature split value
	<li>Leaf node purity
	<li>Leaf node prediction value
	<li>Numbers of samples in decision and leaf nodes
	<li>How a specific feature vector is run down the tree to a leaf
</ul>

<section title="Gallery of decision tree visualizations">
	
Before digging into the previous state-of-the-art visualizations, we'd like to show you what's possible. This section highlights some samples visualizations we built from scikit regression and classification decision trees on classic and other data sets. You can also check out the	
[full gallery](https://github.com/parrt/animl/tree/master/testing/samples) 
and [code to generate all samples](https://github.com/parrt/animl/blob/master/testing/gen_samples.py).

<table>
	<tr>
		<th width="50%"><th width="50%">
	<tr>
		<td>[WINE](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine) 3-class top-down orientation
		<td>[BREAST CANCER](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) 2-class right-to-left
	<tr>
		<td>[<img src="images/icons/wine-TD-2-icon.png" width="100%">](images/wine-TD-2.png)
		<td>[<img src="images/icons/breast_cancer-LR-3-icon.png" width="100%">](images/breast_cancer-LR-3.png)
	<tr>
		<td>[IRIS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) 3-class with sample X
		<td>[USER KNOWLEDGE RATING](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling) 4-class
	<tr>
		<td>[<img src="images/icons/iris-TD-3-X-icon.png" width="100%">](images/iris-TD-3-X.png)
		<td>[<img src="images/icons/knowledge-LR-3-icon.png" width="100%">](images/knowledge-LR-3.png)
	<tr>
		<td>[DIGITS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) 10-class 
		<td>[DIABETES](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) with sample X
	<tr>
		<td>[<img src="images/icons/digits-LR-3-icon.png" width="100%">](images/digits-LR-3.png)
		<td>[<img src="images/icons/diabetes-TD-3-X-icon.png" width="100%">](images/diabetes-TD-3-X.png)	
	<tr>
		<td>[BOSTON](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) with sample X
		<td>[SWEETS](http://mldata.org/repository/data/viewslug/ratings-of-sweets-sweetrs/) with sample X
	<tr>
		<td>[<img src="images/icons/boston-TD-3-X-icon.png" width="100%">](images/boston-TD-3-X.png)
		<td>[<img src="images/icons/sweets-TD-3-X-icon.png" width="100%">](images/sweets-TD-3-X.png)
	<tr>
		<td>USER KNOWLEDGE RATING 4-class non-fancy
		<td>DIABETES non-fancy
	<tr>
		<td>[<img src="images/icons/knowledge-TD-4-simple-icon.png" width="100%">](images/knowledge-TD-4-simple.png)
		<td>[<img src="images/icons/diabetes-TD-4-simple-icon.png" width="100%">](images/diabetes-TD-4-simple.png)



</table>	

<section title="A comparison to previous state-of-the-art visualizations">
	
If you do a web search for "visualizing decision trees" you will quickly find a **Python** solution provided by the awesome scikit folks: [sklearn.tree.export_graphviz](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html).  With more work, you can find visualizations for **R** and even **SAS** and **IBM's Watson**. In this section, we collect the various decision tree visualizations we could find and compare them to the visualizations made by our `animl` library. We give a more detailed discussion of our visualizations in the next section.


Let's start with the [default scitkit visualization](http://scikit-learn.org/stable/modules/tree.html) of a decision tree on the [IRIS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) data set (click on images to enlarge).

<table>
	<tr>
		<th width="50%">
		<th width="50%">
			<tr>
				<td>Default scikit IRIS visualization
				<td>Our `animl` IRIS visualization
	<tr>
		<td>[<img src="images/iris-scikit.png" width="100%">](images/iris-scikit.png)
		<td>[<img src="images/iris-TD-5.png" width="100%">](images/iris-TD-5.png)
</table>
		
The scikit tree does a good job of representing the tree structure, but we have a few quibbles.  The colors aren't the best and it's not immediately obvious why some of the nodes are colored and some aren't.  If the colors represent predicted class for this classifier, one would think just the leaves would be colored because only leaves have predictions.  Including the gini coefficient costs space and doesn't really help the beginners. The count of samples of the  various target classes in each node is very useful, though, a histogram would be even better. A target class color legend would be nice.  Finally, using true and false as the edge labels isn't as clear as, say, labels $<$ and $\ge$.

Scikit uses the same visualization approach for decision tree regressors. For example, here is scikit's visualization using the [BOSTON](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) data set, with `animl`'s for comparison (click to enlarge images):

<table>
	<tr>
		<th width="50%">
		<th width="50%">
			<tr>
				<td>Default scikit BOSTON visualization
				<td>Our `animl` BOSTON visualization
	<tr>
		<td>[<img src="images/boston-scikit.png" width="100%">](images/boston-scikit.png)
		<td>[<img src="images/boston-TD-3.png" width="100%">](images/boston-TD-3.png)
</table>

In the scikit tree, it's not immediately clear what the use of color implies, but after studying the image, darker images indicate higher predicted target values.

**R** programmers also have access to a package for [visualizing decision trees](http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html), which gives similar results to scikit but with nice edge labels:

[<img src="images/R-tree.png" width="40%">](images/R-tree.png)

If you dig really hard, you will find that **SAS** and **IBM** provide (non-Python-based) decision tree visualizations.  

Starting with SAS, we see that they include a histogram related to the node's sample content and other details:
	
<table>
	<tr>
		<th width="50%">
		<th width="50%">
			<tr>
				<td>SAS visualization
				<td>SAS visualization (best image quality we could find with numeric features)
	<tr>
		<td>[<img src="images/sas-tree.png" width="100%">](images/sas-tree.png)
		<td>[<img src="images/sas-tree2.jpeg" width="100%">](images/sas-tree2.jpeg)
</table>


Showing the size indication of the left and right buckets through edge width is a nice touch. But, those histograms are hard to interpret because they have no X axis. Given that the bars in the histogram chart never overlap, the histogram must actually just be a bar chart and the left-right location has no meaning related to feature space. With numeric features, SAS decision trees also show a histogram, but we think a scatterplot showing feature versus target values is the most useful. These SAS node histograms illustrate just target values, which tells us nothing about how the feature space was split.

Moving on to IBM software, here is a visualization, that also shows node bar charts, from [IBM's Watson analytics](https://www.ibm.com/support/knowledgecenter/en/SS4QC9/com.ibm.solutions.wa_an_overview.2.0.0.doc/wa_discover_viz_expl_insigths_dec_tree.html) (on the [TITANIC](https://www.kaggle.com/c/titanic/data) data set):

[<img src="images/ibm-tree.png" width="40%">](images/ibm-tree.png)

The explicit labels for equality or inequality improve understanding and, like SAS trees, the legend is also helpful.

IBM's earlier **SPSS** product also had decision tree visualizations

<table>
	<tr>
		<th width="50%">
		<th width="50%">
			<tr>
				<td>SPSS visualization
				<td>SPSS visualization
	<tr>
		<td>[<img src="images/spss-tree.png" width="100%">](images/spss-tree.png)
		<td>[<img src="images/spss-tree2.png" width="100%">](images/spss-tree2.png)
</table>

All of the visualizations we encountered from the major players were useful, but we were most inspired by the eye-popping visualizations in [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/), which shows an (animated) decision tree like this:

[<img src="images/vizml-tree.png" width="75%">](images/vizml-tree.png)

This visualization has three unique characteristics over previous work, aside from the animation:

<ul>
	<li>the decision nodes show how the feature space is split
	<li>the split point for decision nodes is shown visually (as a wedge) in the distribution
	<li>the leaf size is proportional to the number of samples in that leaf
</ul>

Unfortunately, that visualization is hardcoded to a specific data set and is not a general library. 

What we could not find, however, was widespread use of visualizations showing how a specific feature vector is run down the tree from root to leaf. (One of the SAS trees appears to highlight a path but we didn't find any other examples from other tools and libraries.)

<section title="Our decision tree visualizations">

While the implementation is virtually identical for both classifier and regressor decision trees, the way we interpret them is very different so our visualizations are distinct for the two cases.

*How are decision trees constructed:* The algorithm tries to find a sequence of feature splits such that very different target values go on each side of split but within each side, the target values are very similar. For regression, "similar" translates to low variance of targets in each left and right buckets of splits. For classification, "similar" means that most of the targets in each bucket are of single class.

	*How are predictions made for new observation from the constructed decision tree:* We sequentially map the path of new observation into either left or right side of splits based in which side do feature values fall in each non-leaf node.

	*What do we want to visualize in a decision tree:* We want to see which individual feature has been split apart in each  non-leaf node. What is the distribution of target variables going both in left and right buckets at each node. What are the final predictions at each leaf node. Sometimes, it is also important to realize which particular features for a specific observation lead decision tree to predict whatever it is predicting for that observation, i.e. prediction path of an individual observation. (edited)
	

	


What's important?  Variable name and split point for decision nodes and  prediction, purity, and number of samples for leaves.
	
<subsection title="Classifier decision trees">
	


	started with simple
	
	[<img src="images/iris-TD-4-simple.png" width="55%">](images/iris-TD-4-simple.png)

IRIS:

[<img src="images/iris-TD-5.png" width="75%">](images/iris-TD-5.png)

[USER KNOWLEDGE](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)
	
[<img src="images/knowledge-TD-4.png" width="95%">](images/knowledge-TD-4.png)

when there are too many classes and starts to break down and so we set the limit at 10.
 [DIGITS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)

[<img src="images/digits-TD-2.png" width="65%">](images/digits-TD-2.png)

Subtleties: height of bar graphs, size of leaves, x/y lim as start/stop, hand-picked colors. why pie chart for classification. label edges just once.  legend. thin edges and other lines are hairlines.  consistent y axis for regression trees. alpha set so density becomes darker.  show the split point line and the means of the two halves.  strip plot for relief of regression note shows distribution, mean, size (more/fewer dots) at a glance thanks to strip plot.

<subsection title="Regressor decision trees">

What's important? leaves:  prediction, purity, sample size as before but we need a new visualization other than pie chart. For decision nodes, we need variable name and split point. we also want to see what the distribution is.

	started with simple
	
	[<img src="images/boston-TD-4-simple.png" width="100%">](images/boston-TD-4-simple.png)

	BOSTON

	[<img src="images/boston-TD-3.png" width="75%">](images/boston-TD-3.png)

<section title="Visualizing tree interpreter for single instance">

subtleties: highlight the edges of paths taken, highlight the decision nodes and leaves. highlight the features used to make the decision.  if too many features, then subset to just those features used. difference depending on whether it's left to right or top down. orange wedge showing left/right of split point.
	
	KNOWLEDGE
	
	[<img src="images/class-demo.png" width="75%">](images/class-demo.png)

	[DIABETES](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)

	[<img src="images/diabetes-TD-3-X.png" width="75%">](images/diabetes-TD-3-X.png)

<section title="Left-to-right layout">
	
	
	<section title="Code sample">
		
```
regr = tree.DecisionTreeRegressor(max_depth=max_depth)
boston = load_boston()
regr = regr.fit(boston.data, boston.target)
```

```
st = dtreeviz(regr, boston.data, boston.target, target_name='price',
              feature_names=boston.feature_names)
```

```
g = graphviz.Source(st, format='pdf')
g.render(directory=".", filename="boston.pdf", view=False, cleanup=True)
```

```
g = graphviz.Source(st, format='pdf')
g.view()
```

```
X = boston.data[np.random.randint(0, len(boston.data)),:]

st = dtreeviz(regr, boston.data, boston.target, target_name='price',
              feature_names=boston.feature_names,
              X=X)
```

for classifier we need the class names too.

<section title="Our implementation">
	
	shadow tree
	
	graphviz/dot

	matplotlib

	load svg was issue

	then must parse svg to get size

	future: bottom-justify histograms in classifier trees. some wedge labels overlap with axis. thicken incoming edges for nodes with lots of samples.
	
<section title="What we tried and rejected">

classifier decision nodes

	<img src="images/kde.png" width="50%">
	
	<img src="images/bubble.png" width="70%">

regression leaf nodes		

	<img src="images/dual-leaf.png" width="15%">
		
	<img src="images/non-strip-plot.png" width="30%">