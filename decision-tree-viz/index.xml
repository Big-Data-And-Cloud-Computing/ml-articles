<chapter title="How to visualize decision tree models"
         author={[Terence Parr](http://parrt.cs.usfca.edu) and [Prince Grover](https://www.linkedin.com/in/groverpr)}>


Decision trees are the fundamental building block of [gradient boosting machines](http://explained.ai/gradient-boosting/index.html) and [Random Forests](https://en.wikipedia.org/wiki/Random_forest)\symbol{tm}, probably the two most popular machine learning models for structured data.  Visualizing decision trees is a tremendous aid when learning these models and later, in practice, when interpreting models.  Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. We also couldn't find a package that visualized a specific $\vec{x}$ feature vector being run down the decision tree to the leaf providing the target $\vec{y}$.

Terence has reached a point in his book on [machine learning book](https://mlbook.explained.ai/) (written with [Jeremy Howard](http://www.fast.ai/about/#jeremy)) where he needs to describe lots of decision trees. So, he and former [University of San Francisco MS in Data Science](https://www.usfca.edu/arts-sciences/graduate-programs/data-science) student Prince Grover teamed up to create a general package for [scikit-learn](https://github.com/scikit-learn/scikit-learn) decision tree visualization and model interpretation. This article demonstrates the results of this work, details the specific choices we made for visualization, and outlines the mashup of tools and techniques used in the implementation.

We assume you're familiar with the basic mechanism of decision trees if you're interested in visualizing them, but let's start with a brief summary so that we're all using the same terminology.

<section title="Decision tree review">

A decision tree is a machine learning model based upon binary trees, trees with at most a left and right child.  A decision tree learns the relationship between feature vectors $\vec{x}$ and target values $y$ (in a training set) by examining and condensing training data into a binary tree of interior decision nodes and leaf prediction nodes.  

Each leaf in the decision tree is responsible for making a specific prediction.  If this is a regression tree, the prediction is a value in the target space, such as price.  If this is a classifier tree, the prediction is an integer indicating the predicted target class, such as cancer or not-cancer. A decision tree carves up the feature space into groups of feature vectors that share similar target values and each leaf represents one of these groups.  For regression, "similar" means a low variance between target values and, for classification, "similar" means that most or all targets are of a single class.

Any path from the root of the decision tree to a specific $y$ leaf predictor passes through a series of (internal) decision nodes. Each decision node compares a single feature's value in $\vec{x}$, $x_i$, with a specific \first{split point} value learned during training. For example, in a model predicting apartment rent price, decision nodes would test features such as the number of bedrooms and number of bathrooms.  In a classifier with discrete target values, decision node still compare numeric *feature* values. Most decision tree implementation's assume that all features are numeric, with all categorical variables [one hot encoded](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/) or [binned](https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/) or [label encoded](http://forums.fast.ai/t/to-label-encode-or-one-hot-encode/6057).

To train a decision node, the model examines a subset of the training observations (or the full training set at the root). The node's feature and split point within that feature space are chosen during training to split the observations into left and right buckets (subsets). The left bucket has observations whose $x_i$ feature values are all less than the split point and the right bucket has observations whose $x_i$ is greater than the split point.  The goal is to pick a feature and split point in that feature space so that, within the left and right buckets, the observations have similar target values. (This selection process is generally done through exhaustive comparison of features and  feature values.) Tree construction proceeds recursively by creating decision nodes for the left bucket and the right bucket.  Given a bucket with suitably similar target values, the model creates a leaf node rather than a decision node. The leaf predicts the mean (regression) or most common target (classification).

<section title="Gallery of decision tree visualizations">
	
Before digging into the previous state-of-the-art visualizations, we'd like to show you what's possible. This section highlights some samples visualizations we built from scikit regression and classification decision trees on classic and other data sets. You can also check out the	
[full gallery](https://github.com/parrt/animl/tree/master/testing/samples) 
and [code to generate all samples](https://github.com/parrt/animl/blob/master/testing/gen_samples.py).

<table>
	<tr>
		<th width="50%"><th width="50%">
	<tr>
		<td>[WINE](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine) 3-class top-down orientation
		<td>[BREAST CANCER](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) 2-class right-to-left
	<tr>
		<td>[<img src="images/icons/wine-TD-2-icon.png" width="100%">](images/wine-TD-2.png)
		<td>[<img src="images/icons/breast_cancer-LR-3-icon.png" width="100%">](images/breast_cancer-LR-3.png)
	<tr>
		<td>[IRIS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) 3-class with sample X
		<td>[USER KNOWLEDGE RATING](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling) 4-class
	<tr>
		<td>[<img src="images/icons/iris-TD-3-X-icon.png" width="100%">](images/iris-TD-3-X.png)
		<td>[<img src="images/icons/knowledge-LR-3-icon.png" width="100%">](images/knowledge-LR-3.png)
	<tr>
		<td>[DIGITS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) 10-class 
		<td>[DIABETES](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) with sample X
	<tr>
		<td>[<img src="images/icons/digits-LR-3-icon.png" width="100%">](images/digits-LR-3.png)
		<td>[<img src="images/icons/diabetes-TD-3-X-icon.png" width="100%">](images/diabetes-TD-3-X.png)	
	<tr>
		<td>[BOSTON](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) with sample X
		<td>[SWEETS](http://mldata.org/repository/data/viewslug/ratings-of-sweets-sweetrs/) with sample X
	<tr>
		<td>[<img src="images/icons/boston-TD-3-X-icon.png" width="100%">](images/boston-TD-3-X.png)
		<td>[<img src="images/icons/sweets-TD-3-X-icon.png" width="100%">](images/sweets-TD-3-X.png)
	<tr>
		<td>USER KNOWLEDGE RATING 4-class non-fancy
		<td>DIABETES non-fancy
	<tr>
		<td>[<img src="images/icons/knowledge-TD-4-simple-icon.png" width="100%">](images/knowledge-TD-4-simple.png)
		<td>[<img src="images/icons/diabetes-TD-4-simple-icon.png" width="100%">](images/diabetes-TD-4-simple.png)



</table>	

Sample code.


<section title="Current state-of-the-art visualizations">
	
If you do a web search for "visualizing decision trees" you will quickly find a visualization for **Python** provided by the awesome scikit folks themselves: [sklearn.tree.export_graphviz](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html).  For example, here is [their visualization](http://scikit-learn.org/stable/modules/tree.html) of a decision tree on the [IRIS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) data set (click on image to enlarge).

[<img src="images/iris-scikit.png" width="60%">](images/iris.png)

This image does a good job of representing the tree structure, but we have a few quibbles.  The colors aren't the best and it's not immediately obvious why some of the nodes are colored and some aren't.  If the colors represent predicted class for this classifier, one would think just the leaves would be colored because only leaves have predictions.  Including the gini coefficient costs space and doesn't really help the beginners. The count of samples of the  various target classes in each node is very useful, though, a histogram would be even better. A target class color legend would be nice.  Finally, using true and false as the edge labels isn't as clear as, say, labels \< and >=.

The same mechanism works for decision tree regressors. For example, here is a visualization using the [BOSTON](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) data set (click to enlarge image):

[<img src="images/boston-scikit.png" width="70%">](images/boston-scikit.png)

In this image, it's not immediately clear what the use of color implies, but after studying the image, darker images indicate higher predicted target values.

**R** programmers also have access to a package for [visualizing decision trees](http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html), which gives similar results but with nice edge labels:

[<img src="images/R-tree.png" width="40%">](images/R-tree.png)

If you dig really hard, you will find that **SAS** and **IBM** provide (non-Python-based) decision tree visualizations.  For example, the visualization from SAS includes some interesting details:
	
[<img src="images/sas-tree.png" width="40%">](images/sas-tree.png)

The histogram at each node is a nice touch.  Here's an example visualization from [IBM's Watson analytics](https://www.ibm.com/support/knowledgecenter/en/SS4QC9/com.ibm.solutions.wa_an_overview.2.0.0.doc/wa_discover_viz_expl_insigths_dec_tree.html) that appears to be on the [TITANIC](https://www.kaggle.com/c/titanic/data) data set:

[<img src="images/ibm-tree.png" width="50%">](images/ibm-tree.png)

The explicit labels for equality or inequality improve understanding and, like the SAS tree, the legend is also helpful.

All of these visualizations are useful, but we weren't truly inspired until we saw the impressive [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/), which shows an (animated) decision tree like this:

[<img src="images/vizml-tree.png" width="75%">](images/vizml-tree.png)

Unfortunately, that visualization is hardcoded to a specific data set and is not a general library. It has two unique characteristics, though, aside from the animation: the leaf size is proportional to the number of samples in that leaf and the split point for decision nodes is shown visually in the histogram/distribution.


<section title="Our decision tree visualizations">

While the implementation is virtually identical for both classifier and regressor decision trees, the way we interpret them is very different so our visualizations are distinct for the two cases.

*How are decision trees constructed:* The algorithm tries to find a sequence of feature splits such that very different target values go on each side of split but within each side, the target values are very similar. For regression, "similar" translates to low variance of targets in each left and right buckets of splits. For classification, "similar" means that most of the targets in each bucket are of single class.

	*How are predictions made for new observation from the constructed decision tree:* We sequentially map the path of new observation into either left or right side of splits based in which side do feature values fall in each non-leaf node.

	*What do we want to visualize in a decision tree:* We want to see which individual feature has been split apart in each  non-leaf node. What is the distribution of target variables going both in left and right buckets at each node. What are the final predictions at each leaf node. Sometimes, it is also important to realize which particular features for a specific observation lead decision tree to predict whatever it is predicting for that observation, i.e. prediction path of an individual observation. (edited)
	

	


What's important?  Variable name and split point for decision nodes and  prediction, purity, and number of samples for leaves.
	
<subsection title="Classifier decision trees">
	


	started with simple
	
	[<img src="images/iris-TD-4-simple.png" width="55%">](images/iris-TD-4-simple.png)

IRIS:

[<img src="images/iris-TD-5.png" width="75%">](images/iris-TD-5.png)

[USER KNOWLEDGE](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)
	
[<img src="images/knowledge-TD-4.png" width="95%">](images/knowledge-TD-4.png)

when there are too many classes and starts to break down and so we set the limit at 10.
 [DIGITS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)

[<img src="images/digits-TD-2.png" width="65%">](images/digits-TD-2.png)

Subtleties: height of bar graphs, size of leaves, x/y lim as start/stop, hand-picked colors. why pie chart for classification. label edges just once.  legend. thin edges and other lines are hairlines.  consistent y axis for regression trees. alpha set so density becomes darker.  show the split point line and the means of the two halves.  strip plot for relief of regression note shows distribution, mean, size (more/fewer dots) at a glance thanks to strip plot.

<subsection title="Regressor decision trees">

What's important? leaves:  prediction, purity, sample size as before but we need a new visualization other than pie chart. For decision nodes, we need variable name and split point. we also want to see what the distribution is.

	started with simple
	
	[<img src="images/boston-TD-4-simple.png" width="100%">](images/boston-TD-4-simple.png)

	BOSTON

	[<img src="images/boston-TD-3.png" width="75%">](images/boston-TD-3.png)

<section title="Visualizing tree interpreter for single instance">

subtleties: highlight the edges of paths taken, highlight the decision nodes and leaves. highlight the features used to make the decision.  if too many features, then subset to just those features used. difference depending on whether it's left to right or top down. orange wedge showing left/right of split point.
	
	KNOWLEDGE
	
	[<img src="images/class-demo.png" width="75%">](images/class-demo.png)

	[DIABETES](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)

	[<img src="images/diabetes-TD-3-X.png" width="75%">](images/diabetes-TD-3-X.png)

<section title="Left-to-right layout">
	
	
	<section title="Code sample">
		
```
regr = tree.DecisionTreeRegressor(max_depth=max_depth)
boston = load_boston()
regr = regr.fit(boston.data, boston.target)
```

```
st = dtreeviz(regr, boston.data, boston.target, target_name='price',
              feature_names=boston.feature_names)
```

```
g = graphviz.Source(st, format='pdf')
g.render(directory=".", filename="boston.pdf", view=False, cleanup=True)
```

```
g = graphviz.Source(st, format='pdf')
g.view()
```

```
X = boston.data[np.random.randint(0, len(boston.data)),:]

st = dtreeviz(regr, boston.data, boston.target, target_name='price',
              feature_names=boston.feature_names,
              X=X)
```

for classifier we need the class names too.

<section title="Our implementation">
	
	shadow tree
	
	graphviz/dot

	matplotlib

	load svg was issue

	then must parse svg to get size

	future: bottom-justify histograms in classifier trees. some wedge labels overlap with axis. thicken incoming edges for nodes with lots of samples.
	
<section title="What we tried and rejected">

classifier decision nodes

	<img src="images/kde.png" width="50%">
	
	<img src="images/bubble.png" width="70%">

regression leaf nodes		

	<img src="images/dual-leaf.png" width="15%">
		
	<img src="images/non-strip-plot.png" width="30%">