<chapter title="A visual explanation of L1 Lasso and L2 Ridge regularization for linear models"
	 author={[Terence Parr](http://parrt.cs.usfca.edu)}>

Linear and logistic regression models are interpretable, fast, and form the basis of deep learning neural networks.  They are also extremely simple; we're just fitting lines through training data. Unfortunately, linear models have a tendency to chase outliers in the training data, which often leads to models that don't generalize well to new data. To produce models that generalize better, we all know to *regularize* our models.  There are many forms of regularization, such as *early stopping* and *dropout* for deep learning, but for single linear models, *Lasso* and *Ridge* regularization are most common. (We'll call Lasso *L1* and Ridge *L2* for reasons that'll become clear later.)

My goal in this article is to explain the key intuition that helped me understand exactly how regularization works for linear models. Here it is upfront: **the math used to implement regularization does not correspond to the pictures used to explain regularization**. Take a look at the oft-copied picture on page 71 "Shrinkage Methods" from *The Elements of Statistical Learning* (Hastie, Tibshirani, Friedman):

<img src="images/ESL_reg.png" width="40%">
	
Students see this multiple times in their careers but have trouble mapping that to the relatively straightforward mathematics used to regularize linear model training. The simple reason is that those graphs show how we  regularize models conceptually, with *hard constraints*, not how we actually implement regularization, with *soft constraints*! We'll go into that in detail shortly.  This single disconnect has no doubt caused untold amounts of consternation for those trying to deeply understand regularization. Read on to learn the real story.

I also want to answer key questions regarding L1 Lasso regularization: 

<ol>
<li> *Does L1 encourage model coefficients to shrink to zero or does it simply not discourage zeroes?* (**It encourages zeros.**)
<li> *If L1 encourages zero coefficients, why/how does it do that?!* (**The answer requires a picture, see below.**)
</ol>

These are not easy questions to answer, even for mathematicians. Try explaining simply, without handwaving, to an inquisitive and persistent student; you'll find that you're not exactly sure. ;)

<section title="Review of linear regression">

*going to ignore logistic*
	
For example, here's the single-variable linear regression model familiar to us from high school algebra: $y = \beta_0 + \beta_1 x$, where (coefficient) $\beta_0$ is the $y$-intercept and $\beta_1$ is the slope of the line. 
	 
<section title="Motivation">

linear models Simple, interpretable, super fast. Combining multiple linear models into a lattice with a nonlinear function as glue yields a neural network; those are insanely useful and powerful. Logistic regression model is a 1-neuron neural network

```
lm = LinearRegression()
lm.fit(X, y)
y_pred = lm.predict(X_test)
```

```
lm = Lasso(alpha=45)
lm.fit(X, y)
y_pred = lm.predict(X_test)
```

important to deep learning. here is tensorflow keras
		
```
layer = layers.Dense(100, activation='relu', activity_regularizer=regularizers.l1(0.05)
model.add(layer)
```
	
<pyeval label=reg hide=true output="df">
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib.patches import Circle
import mpl_toolkits.mplot3d.art3d as art3d
from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression

df = pd.DataFrame(data=[[0.0,1.053880874257158],
[1.1111111111111112,1.6930723862524246],
[2.2222222222222223,-0.04867559455082526],
[3.3333333333333335,2.5201150366216343],
[4.444444444444445,4.978339964087746],
[5.555555555555555,5.78858680886268],
[6.666666666666667,7.023970174897514],
[7.777777777777779,6.026499123031133],
[8.88888888888889,9.58117222322382],
[10.0,10.762637572334718]], columns=['x','y'])
</pyeval>

<table>
<tr>
	<td><img src="images/ols.svg" width="85%">
	<td><img src="images/ols_outlier.svg" width="85%">
	<td><img src="images/lasso.svg" width="85%">
</table>
	

	
<section title="How regularization works conceptually">


	
	Lasso: "least absolute shrinkage and selection operator"
	
	why ridge?  the original paper says it's because the surface plots of loss functions can look like ridges, due to the  quadratic nature.  for more good-natured poking of fun see [Statisticians say the darndest things](https://explained.ai/statspeak/index.html)
	

<img src="images/reg1D.svg" width="50%">
	
<img src="images/reg2D.svg" width="50%">

(click to enlarge)

<table>
<tr><th>(a)<th>(b)<th>(c)<th>(d)
<tr>
	<td><img src="images/l2-frame-0.svg" width="100%">
	<td><img src="images/l2-frame-1.svg" width="100%">
	<td><img src="images/l2-frame-2.svg" width="100%">
	<td><img src="images/l2-frame-3.svg" width="100%">
</table>

<table>
<tr><th>(a)<th>(b)<th>(c)<th>(d)
<tr>
	<td><img src="images/l1-frame-0.svg" width="100%">
	<td><img src="images/l1-frame-1.svg" width="100%">
	<td><img src="images/l1-frame-2.svg" width="100%">
	<td><img src="images/l1-frame-3.svg" width="100%">
</table>

<table>
<tr>
	<td><img src="images/l1-symmetric-cloud.png" width="60%">
	<td><img src="images/l2-symmetric-cloud.png" width="60%">
</table>

<table>
<tr>
	<td><img src="images/l1-cloud.png" width="60%">
	<td><img src="images/l2-cloud.png" width="60%">
</table>

<table>
<tr>
	<td><img src="images/l1-orthogonal-cloud.png" width="60%">
	<td><img src="images/l2-orthogonal-cloud.png" width="60%">
</table>

	
<section title="Resources">

	[Deep Learning Basics Lecture 3: Regularization I (slides)](https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/DL_lecture3_regularization_I.pdf) by Yingyu Liang at  Princeton University.
	
[Regularized Regression](https://uc-r.github.io/regularized_regression) from University of Cincinnati.

[Lecture notes on ridge regression](https://arxiv.org/pdf/1509.09169.pdf) by Wessel N. van Wieringen.

[Ridge Regression: Biased Estimation for Nonorthogonal Problems](https://www.math.arizona.edu/~hzhang/math574m/Read/RidgeRegressionBiasedEstimationForNonorthogonalProblems.pdf) by Hoerl and Kennard, Journal Technometrics, 1970.

[ Regression Shrinkage and Selection via the Lasso](http://www-stat.stanford.edu/~tibs/lasso/lasso.pdf) by Tibshirani in Journal of the Royal Statistical Society, 1996.