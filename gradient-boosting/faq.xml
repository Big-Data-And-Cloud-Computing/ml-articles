<chapter title="Gradient boosting: frequently asked questions"
	 author={[Terence Parr](http://parrt.cs.usfca.edu) and [Jeremy Howard](http://www.fast.ai/about/#jeremy)}>
	 
We tried to collect a number of common questions on this page. Please note that we focus on gradient boosting for regression not classification.  We assume that you've read the main articles associated with this FAQ.

<section title="Who invented gradient boosting machines?">

Jerome Friedman, in his seminal paper from 1999 (updated in 2001) called [Greedy  Function Approximation: A  Gradient Boosting Machine](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf), introduced the gradient boosting machine, though the idea of [boosting](https://en.wikipedia.org/wiki/Boosting_(meta-algorithm\)) itself was not new.

<section title="What's the basic idea behind gradient boosting?">

Instead of creating a single powerful model, [Boosting](https://en.wikipedia.org/wiki/Boosting_(meta-algorithm\)) combines multiple simple models into a single composite model. The idea is that, as we introduce more simple models, the overall model becomes a stronger and stronger predictor. In boosting terminology, the simple models are called *weak models* or *weak learners*.

To improve its predictions, gradient boosting looks at the difference between its current approximation, $\hat{\vec y}$ and the known correct target vector, $\vec y$, which is called the residual, $\vec y - \hat{\vec y}$. It then trains a weak model that maps feature vector $\vec x$ to that residual vector.  Adding a residual predicted by a weak model to an existing model's approximation, nudges the model towards the correct target. Adding lots of these nudges, improves the overall models approximation.

<section title="Where does the gradient appear in gradient boosting?">

Residuals contain direction not just magnitude information and so residuals are vectors. The sign of a residual is also a vector.  Most articles treat residuals as the "error" between the approximation and the true target but we generically called these direction vectors to emphasize the fact that they are vectors, not just magnitudes. Considering the residual as an error vector also makes it awkward to talk about gradient boosting machines that optimize the absolute error, not the squared error.

If a vector is a vector of partial derivatives of some function, then that vector is called a gradient. The residual $\vec y - \hat{\vec y}$ is the gradient of $L_2$ loss function $\sum_i^N(y_i-\hat y_i)^2$ and the sign of the residual, $sign(\vec y - \hat{ \vec y})$, is the gradient of $L_1$ loss function $\sum_i^N|y_i-\hat y_i|$.

By adding in approximations to residuals, gradient boosting machines are chasing gradients, hence, the term *gradient* boosting.

<section title="What is the difference between gradient boosting optimizing squared error versus absolute error?">
	
GBMs that optimize MSE ($L_2$ loss) and MAE ($L_1$ loss) both train regression trees, $\Delta_m$, on direction vectors, but MSE-optimizing GBMs train trees on residual vectors and MAE GBMs train trees on sign vectors. The goal of training regression tree models is to group similar direction vectors into leaf nodes in both cases.  Because they are training on different vectors (residuals versus signs), the trees will group the observations in the training data differently.

Both MSE and MAE GBM regression tree leaves predict residuals, but MSE GBM leaves predict the average residual while MAE GBM leaves predict the median residual.

<section title="What is function space?">

Function space is just the $N$ space of predictions associated with $N$ observations. Each point in function space, or prediction spaces we like to call it, is just to the output of calling function $F_m(X)$ on observation matrix $X = [\vec x_1, \vec x_2, ..., \vec x_n]$.

<section title="How is gradient boosting performing gradient descent in function space?">

First, simplify "function space" to just "prediction space" and then let $\hat{\vec y}$  be our current prediction vector in prediction space.  As we nudge $\hat{\vec y}$ towards the true target, we add residual vectors. In the [last article in this series](descent.html), we showed that the residual vector is a gradient vector.  Adding gradient vectors is the same as subtracting the negative of the gradient, which led us to equate the gradient descent position update equation and the gradient boosting model update equation:

<table>
<tr>
	<th>Gradient descent
	<th>Gradient boosting
<tr>
	<td>$\vec x_t = \vec x_{t-1} - \eta \nabla f(\vec x_{t-1})$
	<td>$\hat{\vec y}_m = \hat{\vec y}_{m-1} + \eta (- \nabla L(\vec y, \hat{\vec y}_{m-1}))$
</table>

In a nutshell, chasing the direction vector, residual or sign vector, chases the (negative) gradient of a loss function just like gradient descent.

<section title="We train weak models on residuals y-y_hat so why don't we need y when using the model on an unknown feature vector?">
		
do we need the true target in the test set? no, the model is no longer updated after training and we only need the true target to compute new weak models during training.

<section title="How is gradient boosting different from our typical usage of gradient descent?">
	
normally we are shifting model parameters around to optimize a model. In this case we are moving the approximation around in the direction of the answer. We are not optimizing the parameters of the week models.  gradient descent does parameter optimization normally but we are now doing function space optimization. Gradient descent can't be doing parameter optimization because the different kinds of models would have different parameters. Friedman says '*... consider F(x) evaluated at each point $\vec x$ to be a "parameter"*'	

why approx residual? rpevent overfit. won't generalize. adding delta adds in residual or sign. sum together.


<section title="Can you point me at a resource for gradients and matrix calculus?">
	
Sure, check out [The Matrix Calculus You Need For Deep Learning](http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html).

