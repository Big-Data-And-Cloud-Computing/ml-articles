<chapter title="How to visualize decision tree models"
         author={[Terence Parr](http://parrt.cs.usfca.edu) and [Prince Grover](https://www.linkedin.com/in/groverpr)}>


Decision trees are the fundamental building block of [gradient boosting machines](http://explained.ai/gradient-boosting/index.html) and [Random Forests](https://en.wikipedia.org/wiki/Random_forest)\symbol{tm}, probably the two most popular machine learning models for structured data.  Visualizing decision trees is a tremendous aid when learning these models and later, in practice, when interpreting models.  Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice.

Terence has reached a point in his book on [machine learning book](https://mlbook.explained.ai/) (written with [Jeremy Howard](http://www.fast.ai/about/#jeremy)) where he needs to describe lots of decision trees. So, he and former [University of San Francisco MS in Data Science](https://www.usfca.edu/arts-sciences/graduate-programs/data-science) student Prince Grover teamed up to create a general package for [scikit-learn](https://github.com/scikit-learn/scikit-learn) decision tree visualization and model interpretation. This article demonstrates the results of this work, details the specific choices we made for visualization, and outlines the mashup of tools and techniques used in the implementation. The visualization software is part of a nascent Python machine learning library called [animl](https://github.com/parrt/animl).

We assume you're familiar with the basic mechanism of decision trees if you're interested in visualizing them, but let's start with a brief summary so that we're all using the same terminology.

<section title="Decision tree review">

A decision tree is a machine learning model based upon binary trees, trees with at most a left and right child.  A decision tree learns the relationship between feature vectors $\vec{x}$ and target values $y$ (in a training set) by examining and condensing training data into a binary tree of interior decision nodes and leaf prediction nodes.  

Each leaf in the decision tree is responsible for making a specific prediction.  If this is a regression tree, the prediction is a value in the target space, such as price.  If this is a classifier tree, the prediction is an integer indicating the predicted target class, such as cancer or not-cancer. A decision tree carves up the feature space into groups of feature vectors that share similar target values and each leaf represents one of these groups.  For regression, "similar" means a low variance between target values and, for classification, "similar" means that most or all targets are of a single class.

Any path from the root of the decision tree to a specific $y$ leaf predictor passes through a series of (internal) decision nodes. Each decision node compares a single feature's value in $\vec{x}$, $x_i$, with a specific \first{split point} value learned during training. For example, in a model predicting apartment rent price, decision nodes would test features such as the number of bedrooms and number of bathrooms.  In a classifier with discrete target values, decision node still compare numeric *feature* values. Most decision tree implementation's assume that all features are numeric, with all categorical variables [one hot encoded](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/) or [binned](https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/) or [label encoded](http://forums.fast.ai/t/to-label-encode-or-one-hot-encode/6057).

To train a decision node, the model examines a subset of the training observations (or the full training set at the root). The node's feature and split point within that feature space are chosen during training to split the observations into left and right buckets (subsets). The left bucket has observations whose $x_i$ feature values are all less than the split point and the right bucket has observations whose $x_i$ is greater than the split point.  The goal is to pick a feature and split point in that feature space so that, within the left and right buckets, the observations have similar target values. (This selection process is generally done through exhaustive comparison of features and  feature values.) Tree construction proceeds recursively by creating decision nodes for the left bucket and the right bucket.  Given a bucket with suitably similar target values, the model creates a leaf node rather than a decision node. The leaf predicts the mean (regression) or most common target (classification).

<section title="The key elements of decision tree visualization">
	
Decision tree visualizations would optimally highlight the following important elements:

<ul>
	<li>Decision node feature vs target value distributions; i.e., how separable are the target values based upon the feature and a split point?
	<li>Decision node feature name
	<li>Decision node feature split value
	<li>Leaf node purity
	<li>Leaf node prediction value
	<li>Numbers of samples in decision and leaf nodes
	<li>How a specific feature vector is run down the tree to a leaf
</ul>

<section title="Gallery of decision tree visualizations">
	
Before digging into the previous state-of-the-art visualizations, we'd like to give a little spoiler to show what's possible. This section highlights some samples visualizations we built from scikit regression and classification decision trees on classic and other data sets. You can also check out the	
[full gallery](https://github.com/parrt/animl/tree/master/testing/samples) 
and [code to generate all samples](https://github.com/parrt/animl/blob/master/testing/gen_samples.py).

<table>
	<tr>
		<th width="50%"><th width="50%">
	<tr>
		<td>[WINE](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine) 3-class top-down orientation
		<td>[BREAST CANCER](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) 2-class right-to-left
	<tr>
		<td>[<img src="images/icons/wine-TD-2-icon.png" width="100%">](images/wine-TD-2.png)
		<td>[<img src="images/icons/breast_cancer-LR-3-icon.png" width="100%">](images/breast_cancer-LR-3.png)
	<tr>
		<td>[IRIS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) 3-class with sample X
		<td>[USER KNOWLEDGE RATING](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling) 4-class
	<tr>
		<td>[<img src="images/icons/iris-TD-3-X-icon.png" width="100%">](images/iris-TD-3-X.png)
		<td>[<img src="images/icons/knowledge-LR-3-icon.png" width="100%">](images/knowledge-LR-3.png)
	<tr>
		<td>[DIGITS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) 10-class 
		<td>[DIABETES](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html) with sample X
	<tr>
		<td>[<img src="images/icons/digits-LR-3-icon.png" width="100%">](images/digits-LR-3.png)
		<td>[<img src="images/icons/diabetes-TD-3-X-icon.png" width="100%">](images/diabetes-TD-3-X.png)	
	<tr>
		<td>[BOSTON](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) with sample X
		<td>[SWEETS](http://mldata.org/repository/data/viewslug/ratings-of-sweets-sweetrs/) with sample X
	<tr>
		<td>[<img src="images/icons/boston-TD-3-X-icon.png" width="100%">](images/boston-TD-3-X.png)
		<td>[<img src="images/icons/sweets-TD-3-X-icon.png" width="100%">](images/sweets-TD-3-X.png)
	<tr>
		<td>USER KNOWLEDGE RATING 4-class non-fancy
		<td>DIABETES non-fancy
	<tr>
		<td>[<img src="images/icons/knowledge-TD-4-simple-icon.png" width="100%">](images/knowledge-TD-4-simple.png)
		<td>[<img src="images/icons/diabetes-TD-4-simple-icon.png" width="100%">](images/diabetes-TD-4-simple.png)



</table>	

<section title="A comparison to previous state-of-the-art visualizations">
	
If you do a web search for "visualizing decision trees" you will quickly find a **Python** solution provided by the awesome scikit folks: [sklearn.tree.export_graphviz](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html).  With more work, you can find visualizations for **R** and even **SAS** and **IBM's Watson**. In this section, we collect the various decision tree visualizations we could find and compare them to the visualizations made by our `animl` library. We give a more detailed discussion of our visualizations in the next section.


Let's start with the [default scitkit visualization](http://scikit-learn.org/stable/modules/tree.html) of a decision tree on the [IRIS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris) data set (click on images to enlarge).

<table>
	<tr>
		<th width="50%">
		<th width="50%">
			<tr>
				<td>Default scikit IRIS visualization
				<td>Our `animl` IRIS visualization
	<tr>
		<td>[<img src="images/iris-scikit.png" width="100%">](images/iris-scikit.png)
		<td>[<img src="images/iris-TD-5.png" width="100%">](images/iris-TD-5.png)
</table>
		
The scikit tree does a good job of representing the tree structure, but we have a few quibbles.  The colors aren't the best and it's not immediately obvious why some of the nodes are colored and some aren't.  If the colors represent predicted class for this classifier, one would think just the leaves would be colored because only leaves have predictions.  Including the gini coefficient costs space and doesn't really help the beginners. The count of samples of the  various target classes in each node is very useful, though, a histogram would be even better. A target class color legend would be nice.  Finally, using true and false as the edge labels isn't as clear as, say, labels $<$ and $\ge$. The most obvious difference is that our decision nodes show feature distributions as overlapping stacked-histograms, one histogram per target class. Leaf size indicates the relative number of samples in the leaves.

Scikit uses the same visualization approach for decision tree regressors. For example, here is scikit's visualization using the [BOSTON](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) data set, with `animl`'s version for comparison (click to enlarge images):

<table>
	<tr>
		<th width="50%">
		<th width="50%">
			<tr>
				<td>Default scikit BOSTON visualization
				<td>Our `animl` BOSTON visualization
	<tr>
		<td>[<img src="images/boston-scikit.png" width="100%">](images/boston-scikit.png)
		<td>[<img src="images/boston-TD-3.png" width="100%">](images/boston-TD-3.png)
</table>

In the scikit tree, it's not immediately clear what the use of color implies, but after studying the image, darker images indicate higher predicted target values. As before, our decision nodes show the feature space distribution, this time using a feature versus target value scatterplot.  The leaves use strip plots to show the target value distribution and leaves with more dots naturally have a higher number of samples.

**R** programmers also have access to a package for [visualizing decision trees](http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html), which gives similar results to scikit but with nice edge labels:

[<img src="images/R-tree.png" width="40%">](images/R-tree.png)

If you dig really hard, you will find that **SAS** and **IBM** provide (non-Python-based) decision tree visualizations.  Starting with SAS, we see that their decision nodes include a bar chart related to the node's sample target values and other details:
	
<table>
	<tr>
		<th width="50%">
		<th width="50%">
			<tr>
				<td>SAS visualization
				<td>SAS visualization (best image quality we could find with numeric features)
	<tr>
		<td>[<img src="images/sas-tree.png" width="100%">](images/sas-tree.png)
		<td>[<img src="images/sas-tree2.jpeg" width="100%">](images/sas-tree2.jpeg)
</table>


Showing the size indication of the left and right buckets through edge width is a nice touch. But, those bar charts are hard to interpret because they have no X axis.  Decision nodes testing categorical variables (left image) have exactly one bar per category so they must represent simple category counts, rather than feature distributions. For numeric features (right image), SAS decision nodes show a histogram of either target  or feature space (we can't tell from the image), but we think a scatterplot showing feature versus target values is the most useful. These SAS node bar charts / histograms illustrate just target values, which tells us nothing about how the feature space was split.

The SAS tree on the right appears to highlight a path through the decision tree for a specific unknown future vector, but we didn't find any other examples from other tools and libraries.  The ability to visualize a specific vector run down the tree does not seem to be generally available.

Moving on to IBM software, here is a nice visualization, that also shows decision node category counts as bar charts, from [IBM's Watson analytics](https://www.ibm.com/support/knowledgecenter/en/SS4QC9/com.ibm.solutions.wa_an_overview.2.0.0.doc/wa_discover_viz_expl_insigths_dec_tree.html) (on the [TITANIC](https://www.kaggle.com/c/titanic/data) data set):

[<img src="images/ibm-tree.png" width="60%">](images/ibm-tree.png)

IBM's earlier **SPSS** product also had decision tree visualizations:

<table>
	<tr>
		<th width="50%">
		<th width="50%">
			<tr>
				<td>SPSS visualization
				<td>SPSS visualization
	<tr>
		<td>[<img src="images/spss-tree.png" width="100%">](images/spss-tree.png)
		<td>[<img src="images/spss-tree2.png" width="100%">](images/spss-tree2.png)
</table>

These SPSS decision nodes seem to give the same SAS-like bar chart of sample target class counts.

All of the visualizations we encountered from the major players were useful, but we were most inspired by the eye-popping visualizations in [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/), which shows an (animated) decision tree like this:

[<img src="images/vizml-tree.png" width="75%">](images/vizml-tree.png)

This visualization has three unique characteristics over previous work, aside from the animation:

<ul>
	<li>the decision nodes show how the feature space is split
	<li>the split point for decision nodes is shown visually (as a wedge) in the distribution
	<li>the leaf size is proportional to the number of samples in that leaf
</ul>

While that visualization is hardcoded to a specific data set for educational purposes, it points us in the right direction.

<section title="Our decision tree visualizations">

just to be clear, decision nodes split feature space to group similar target values into two buckets.  decision nodes that do not show both feature space and target space do not illustrate how the decision node was created.
	
While the implementation is virtually identical for both classifier and regressor decision trees, the way we interpret them is very different so our visualizations are distinct for the two cases.

<subsection title="Classifier decision trees">
	started with simple
	
	[<img src="images/iris-TD-4-simple.png" width="55%">](images/iris-TD-4-simple.png)

IRIS:

[<img src="images/iris-TD-5.png" width="75%">](images/iris-TD-5.png)

[USER KNOWLEDGE](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)
	
[<img src="images/knowledge-TD-4.png" width="95%">](images/knowledge-TD-4.png)

when there are too many classes and starts to break down and so we set the limit at 10.
 [DIGITS](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)

[<img src="images/digits-TD-2.png" width="65%">](images/digits-TD-2.png)

Subtleties: height of bar graphs, size of leaves, x/y lim as start/stop, hand-picked colors. why pie chart for classification. label edges just once.  legend. thin edges and other lines are hairlines.  consistent y axis for regression trees. alpha set so density becomes darker.  show the split point line and the means of the two halves.  strip plot for relief of regression note shows distribution, mean, size (more/fewer dots) at a glance thanks to strip plot.

<subsection title="Regressor decision trees">

What's important? leaves:  prediction, purity, sample size as before but we need a new visualization other than pie chart. For decision nodes, we need variable name and split point. we also want to see what the distribution is.

	started with simple
	
	[<img src="images/boston-TD-4-simple.png" width="100%">](images/boston-TD-4-simple.png)

	BOSTON

	[<img src="images/boston-TD-3.png" width="75%">](images/boston-TD-3.png)

<section title="Visualizing tree interpreter for single instance">

subtleties: highlight the edges of paths taken, highlight the decision nodes and leaves. highlight the features used to make the decision.  if too many features, then subset to just those features used. difference depending on whether it's left to right or top down. orange wedge showing left/right of split point. The X axis feature space dimension for the same feature is using the same xlim so you can see how the nodes split things up; we don't zoom in. Same for price or Y axis.  proportional size of nodes is nonlinear log(sqrt(x)).

Our stacked histogram showing the total not the max for each bar.
	
	KNOWLEDGE
	
	[<img src="images/class-demo.png" width="75%">](images/class-demo.png)

	[DIABETES](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)

	[<img src="images/diabetes-TD-3-X.png" width="75%">](images/diabetes-TD-3-X.png)

<section title="Left-to-right layout">
	
	
	<section title="Code sample">
		
```
regr = tree.DecisionTreeRegressor(max_depth=max_depth)
boston = load_boston()
regr = regr.fit(boston.data, boston.target)
```

```
st = dtreeviz(regr, boston.data, boston.target, target_name='price',
              feature_names=boston.feature_names)
```

```
g = graphviz.Source(st, format='pdf')
g.render(directory=".", filename="boston.pdf", view=False, cleanup=True)
```

```
g = graphviz.Source(st, format='pdf')
g.view()
```

```
X = boston.data[np.random.randint(0, len(boston.data)),:]

st = dtreeviz(regr, boston.data, boston.target, target_name='price',
              feature_names=boston.feature_names,
              X=X)
```

for classifier we need the class names too.

<section title="Our implementation">
	
	shadow tree
	
	graphviz/dot

	matplotlib

	load svg was issue

	then must parse svg to get size

	future: bottom-justify histograms in classifier trees. some wedge labels overlap with axis. thicken incoming edges for nodes with lots of samples.
	
<section title="What we tried and rejected">

classifier decision nodes

	<img src="images/kde.png" width="50%">

	<img src="images/kde-leaf.png" width="15%">
	
	<img src="images/bubble.png" width="70%">

regression leaf nodes		

	<img src="images/dual-leaf.png" width="15%">
		
	<img src="images/non-strip-plot.png" width="30%">
		
<section title="Lessons learned">

We are definitely not visualization aficionados, but for this specific problem we banged on it until we got effective diagrams.

	there are lots of visual effects one can choose from to visualize data: color, line thickness, line style, different kinds of plots, size (area, length, graph height, ...), alpha, text styles (color, font, bold, italics, size), graph annotations, visual flow.  Everything has to be for a reason don't just use color because colors are nice. they have to mean something.  In Tufte's seminar I learned that you can pack a lot of information into a rich diagram; humans can deal with it, but it can't be a mishmash.
	
	as our subtlety list shows, there's more to it.  same axis, same scale etc.