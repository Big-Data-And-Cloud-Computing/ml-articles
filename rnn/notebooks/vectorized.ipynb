{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using vectorized mini-batch SGD\n",
    "\n",
    "This notebook is part of article [Explaining RNNs without neural networks](https://explained.ai/rnn/index.html) and notebook [prep.ipynb](prep.ipynb) should be run before this notebook as it needs files: `data/X.pkl` and `data/y.pkl`.\n",
    "\n",
    "Instead of processing batch one record at a time from time 1 to time len(word), process all time steps t across all batch records at once, then proceed to time step (char index) t+1.  This allows us to vectorize and perform each time step in parallel.  We effectively remove a loop.\n",
    "\n",
    "But, it means we must pad to have same length in batch. We pad on left so the zero vectors are ignored to get same answer as record-by-record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=3000, threshold=20000)\n",
    "from typing import Sequence\n",
    "\n",
    "!if ! test -f support.py; then wget https://raw.githubusercontent.com/parrt/ml-articles/master/rnn/notebooks/support.py; fi\n",
    "\n",
    "from support import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('data/y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TESTING SUBSAMPLE\n",
    "idx = list(np.random.randint(0,len(X),size=2000))\n",
    "X = np.array(X)[idx].tolist()\n",
    "y = np.array(y)[idx].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(X):\n",
    "    max_len = 0\n",
    "    for x in X:\n",
    "        max_len = max(max_len, len(x))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_matrix(X, max_len, vocab, verbose=False):\n",
    "    X_onehot = torch.zeros(len(X),max_len,len(vocab), dtype=torch.float64)\n",
    "    for i,x in enumerate(X):\n",
    "        pad = max_len - len(x)\n",
    "        for j,c in enumerate(x):\n",
    "            X_onehot[i, j+pad, ctoi[c]] = 1\n",
    "        if verbose: print(x); print(X_onehot[i].T, \"\\n\")\n",
    "    return X_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X:Sequence[Sequence], max_len:int, vocab:dict):\n",
    "    \"Cut-n-paste from body of training for use with metrics\"\n",
    "    X_onehot = onehot_matrix(X, max_len, vocab)\n",
    "    h = torch.zeros(nhidden, len(X), dtype=torch.float64, requires_grad=False)\n",
    "    for j in range(max_len):\n",
    "        x_step_t = X_onehot[:,j].T\n",
    "        h = W.mm(h) + U.mm(x_step_t)\n",
    "        h = torch.tanh(h)        \n",
    "    o = V.mm(h)\n",
    "    o = o.T # make it batch_size x nclasses\n",
    "    o = softmax(o)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Just some matrices. First, set up hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ctoi = getvocab(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with trivial data set\n",
    "\n",
    "Set TESTING=True to test vs full X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False\n",
    "\n",
    "nhidden = 100\n",
    "batch_size = 32\n",
    "\n",
    "if TESTING:\n",
    "    nhidden = 2\n",
    "    batch_size = 2\n",
    "\n",
    "    X_train = [['a','b'],['c','d','e'], # batch 1\n",
    "               ['f'],['c','a'], # batch 2\n",
    "               ['e']] # strip\n",
    "    y_train = [0,2,1,1,2]\n",
    "\n",
    "    X_valid = X_train\n",
    "    y_valid = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,688 training records, batch size 32, 29 features (chars), 18 target languages, state is 100-vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parrt/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "n = len(X_train)\n",
    "\n",
    "nbatches = n // batch_size\n",
    "n = nbatches * batch_size\n",
    "X_train = X_train[0:n]\n",
    "y_train = y_train[0:n]\n",
    "vocab, ctoi = getvocab(X_train)\n",
    "max_len = get_max_len(X_train)\n",
    "nfeatures = len(vocab)\n",
    "nclasses = len(torch.unique(torch.tensor(y_train)))\n",
    "\n",
    "print(f\"{n:,d} training records, batch size {batch_size}, {nfeatures} features (chars), {nclasses} target languages, state is {nhidden}-vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['h', 'l', 'e', 'b', 'o', 'd', 'a', 'r', 'o', 'v'],\n",
       " ['g', 'r', 'i', 'l', 'l', 'o']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_onehot = onehot_matrix(X_train, max_len, vocab, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With verbose and trivial X_train we get:\n",
    "\n",
    "```\n",
    "tensor([[[0., 0., 0., 0., 0., 0.],\n",
    "         [1., 0., 0., 0., 0., 0.],\n",
    "         [0., 1., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 1., 0., 0., 0.],\n",
    "         [0., 0., 0., 1., 0., 0.],\n",
    "         [0., 0., 0., 0., 1., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 1.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 1., 0., 0., 0.],\n",
    "         [1., 0., 0., 0., 0., 0.]]], dtype=torch.float64)\n",
    "```\n",
    "\n",
    "With `X_onehot.shape` = [4, 3, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using vectorized minibatch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parrt/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/parrt/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 accum loss  2.0397 accur 0.606 | train loss  1.2226 accur 0.673 | valid loss  1.2030 accur 0.678\n",
      "Epoch:   2 accum loss  1.1713 accur 0.684 | train loss  1.0079 accur 0.718 | valid loss  1.0791 accur 0.705\n",
      "Epoch:   3 accum loss  1.0224 accur 0.711 | train loss  0.9816 accur 0.720 | valid loss  1.0956 accur 0.699\n",
      "Epoch:   4 accum loss  0.9360 accur 0.727 | train loss  0.9181 accur 0.733 | valid loss  1.0706 accur 0.707\n",
      "Epoch:   5 accum loss  0.8655 accur 0.748 | train loss  0.8763 accur 0.742 | valid loss  1.0810 accur 0.701\n",
      "Epoch:   6 accum loss  0.8257 accur 0.756 | train loss  0.9027 accur 0.737 | valid loss  1.1066 accur 0.711\n",
      "Epoch:   7 accum loss  0.7856 accur 0.766 | train loss  0.8087 accur 0.771 | valid loss  1.0510 accur 0.731\n",
      "Epoch:   8 accum loss  0.7898 accur 0.762 | train loss  0.8462 accur 0.751 | valid loss  1.0917 accur 0.717\n",
      "Epoch:   9 accum loss  0.7725 accur 0.768 | train loss  0.8191 accur 0.770 | valid loss  1.0709 accur 0.727\n",
      "Epoch:  10 accum loss  0.7554 accur 0.770 | train loss  0.7448 accur 0.780 | valid loss  1.0291 accur 0.735\n",
      "Epoch:  11 accum loss  0.7575 accur 0.770 | train loss  1.1453 accur 0.701 | valid loss  1.4142 accur 0.665\n",
      "Epoch:  12 accum loss  0.8473 accur 0.756 | train loss  0.7322 accur 0.786 | valid loss  0.9837 accur 0.733\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADUCAYAAABwOKTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfu0lEQVR4nO3deXhU5fnw8e+dmclMQghLRHZZVEABBY2CddcWERf6uiCt1qWLr1oVbV3w7a9ul74/u7za2irUtoK21LpQKiouSFXqhgV/QUA2QZbIlgTJQjLJTHK/f5yTMEkmyQnJTDLk/lzXXOfMWe8cuOc855zneY6oKsaY1JHW0QEYY1rHktaYFGNJa0yKsaQ1JsVY0hqTYixpjUkxCUtaERkpInkxnxIRuS1R+zOmq5BkPKcVER/wFTBBVbcmfIfGHMKSVTw+F9hkCWtM2yUraacDzyVpX8Yc0hJePBaRdGAHMFpVd8eZfz1wPUC3bt1OHDVqVELjMSYVrFixolBV+8Sbl4yknQr8WFUntbRsbm6uLl++PKHxGJMKRGSFqubGm5eM4vF3sKKxMe0moUkrIpnAt4B/JHI/xnQl/kRuXFXLgZxE7sOYriahSWsOTZFIhPz8fMLhcEeHkvJCoRCDBg0iEAh4XseS1rRafn4+3bt3Z+jQoYhIR4eTslSVoqIi8vPzGTZsmOf1rO6xabVwOExOTo4lbBuJCDk5Oa0usVjSmoNiCds+DuY4WtIak2IsaU3K2bdvH08++WSr15syZQr79u1r9XrXXnstL730UqvXSxRLWpNymkra6urqZtdbtGgRPXv2TFRYSWN3j02bPPDKGj7fUdKu2zx2QDb3XTS6yfkzZ85k06ZNjBs3jkAgQFZWFv379ycvL4/PP/+cb3/722zfvp1wOMyMGTO4/vrrARg6dCjLly+nrKyM888/n9NOO40PP/yQgQMH8vLLL5ORkdFibEuWLOGOO+4gGo1y0kknMWvWLILBIDNnzmThwoX4/X4mTZrEr3/9a1588UUeeOABfD4fPXr0YOnSpe1yfCxpTcp55JFHWL16NXl5ebz77rtccMEFrF69uu6xydNPP03v3r2pqKjgpJNO4tJLLyUnp34dn40bN/Lcc8/xxz/+kWnTpjF//nyuuuqqZvcbDoe59tprWbJkCSNGjODqq69m1qxZXH311SxYsIB169YhInVF8AcffJA333yTgQMHHlSxvCmWtKZNmjsjJsvJJ59c7znn448/zoIFCwDYvn07GzdubJS0w4YNY9y4cQCceOKJbNmypcX9rF+/nmHDhjFixAgArrnmGp544gluvvlmQqEQP/zhD7ngggu48MILATj11FO59tprmTZtGpdcckl7/KmAXdOaQ0C3bt3qxt99913efvttPvroI1auXMn48ePjPgcNBoN14z6fj2g02uJ+mmoR5/f7+eSTT7j00kv55z//yeTJkwGYPXs2Dz30ENu3b2fcuHEUFRW19k+Lv7922YoxSdS9e3dKS0vjzisuLqZXr15kZmaybt06Pv7443bb76hRo9iyZQtffPEFRx11FH/5y18488wzKSsro7y8nClTpjBx4kSOOuooADZt2sSECROYMGECr7zyCtu3b290xj8YlrQm5eTk5HDqqacyZswYMjIy6Nu3b928yZMnM3v2bI477jhGjhzJxIkT222/oVCIOXPmcPnll9fdiLrhhhvYu3cvU6dOJRwOo6o89thjANx5551s3LgRVeXcc8/l+OOPb5c4ktKxm1fWCD41rF27lmOOOaajwzhkxDueHdYIXkR6ishLIrJORNaKyCmJ3J8xXUGii8e/Bd5Q1cvcvqIyE7w/Yw7aj3/8Yz744IN602bMmMF1113XQRHFl7CkFZFs4AzgWgBVrQKqErU/Y9rqiSee6OgQPElk8Xg4UADMEZH/EZE/iUi3llYyxjQvkUnrB04AZqnqeGA/MLPhQiJyvYgsF5HlBQUFCQzHmENDIpM2H8hX1WXu95dwkrgeVX1KVXNVNbdPn7jdvBpjYiQsaVV1F7BdREa6k84FPk/U/ozpKhJdjfEWYJ6IfAaMA/5vgvdnTCNZWVlNztuyZQtjxoxJYjRtl+guVPOAuA+IjTEHx6oxmrZ5fSbsWtW+2+w3Fs5/pMnZd999N0OGDOGmm24C4P7770dEWLp0KV9//TWRSISHHnqIqVOntmq34XCYG2+8keXLl+P3+3n00Uc5++yzWbNmDddddx1VVVXU1NQwf/58BgwYwLRp08jPz6e6upqf//znXHHFFW36s72ypDUpZ/r06dx22211SfvCCy/wxhtvcPvtt5OdnU1hYSETJ07k4osvblXHabXPaVetWsW6deuYNGkSGzZsYPbs2cyYMYMrr7ySqqoqqqurWbRoEQMGDOC1114DnIYKyWJJa9qmmTNioowfP549e/awY8cOCgoK6NWrF/379+f2229n6dKlpKWl8dVXX7F792769evnebvvv/8+t9xyC+C06BkyZAgbNmzglFNO4eGHHyY/P59LLrmEo48+mrFjx3LHHXdw9913c+GFF3L66acn6s9txNrTmpR02WWX8dJLL/H8888zffp05s2bR0FBAStWrCAvL4++ffu2uj/hphrPfPe732XhwoVkZGRw3nnn8a9//YsRI0awYsUKxo4dyz333MODDz7YHn+WJ3amNSlp+vTp/OhHP6KwsJD33nuPF154gcMPP5xAIMA777zD1q1bW73NM844g3nz5nHOOeewYcMGtm3bxsiRI9m8eTPDhw/n1ltvZfPmzXz22WeMGjWK3r17c9VVV5GVlcXcuXPb/49sgiWtSUmjR4+mtLSUgQMH0r9/f6688kouuugicnNzGTduHAfzcvKbbrqJG264gbFjx+L3+5k7dy7BYJDnn3+ev/71rwQCAfr168e9997Lf/7zH+68807S0tIIBALMmjUrAX9lfNae1rSatadtX52qPa0xpv1Z8dh0CatWreJ73/tevWnBYJBly5Y1sUbn1WLSisgMYA5QCvwJGA/MVNW3EhybMe1m7Nix5OXldXQY7cJL8fj7qloCTAL6ANcByX84ZzqVznQvJJUdzHH0krS1VUqmAHNUdWXMNNMFhUIhioqKLHHbqPal0qFQqFXrebmmXSEibwHDgHtEpDtQcxAxmkPEoEGDyM/PxzotaLtQKMSgQYNatY6XpP0BTrO6zapaLiK9cYrIposKBAL1XsNhkstL8fgUYL2q7hORq4D/ApJXO9oYU4+XpJ0FlIvI8cBdwFbgWS8bF5EtIrJKRPJExGpNGNMOvBSPo6qqIjIV+K2q/llErmnFPs5W1cKDjM8Y04CXpC0VkXuA7wGni4gPCCQ2LGNMU7wUj68AKnGe1+4CBgK/8rh9Bd4SkRUicn28BawLVWNax1ODARHpC5zkfv1EVfd42rjIAFXdISKHA4uBW1S1yXfYW4MBYxxtajAgItOAT4DLgWnAMhG5zMuOVXWHO9wDLABO9hq0MSY+L9e0PwNOqj27ikgf4G2czseb5L4CJE1VS93xSUDymvcbc4jykrRpDYrDRXi7Fu4LLHA71vIDf1PVN1ofojEmlpekfUNE3gSec79fASxqaSVV3Qy0z6uvjTF1WkxaVb1TRC4FTsVpKPCUqi5IeGTGmLg8NYJX1fnA/ATHYozxoMmkFZFSnOesjWYBqqrZCYvKGNOkJpNWVbsnMxBjjDfWsZsxKcaS1pgUY0lrTIqxpDUmxXjpQjXeXeRiYDnwU7cShTEmSbw8p30U2AH8Dedxz3SgH7AeeBo4K1HBGWMa81I8nqyqf1DVUlUtUdWngCmq+jzQK8HxGWMa8JK0NSIyTUTS3M+0mHnW8a0xSeYlaa/E6Wpmj/v5HnCViGQANycwNmNMHF4aDGwGLmpi9vvtG44xpiVeeq4YJCILRGSPiOwWkfki4rlLdBHxicj/iMirbQvVGAPeisdzgIXAAJxO3V5xp3k1A1jb+tCMMfF4Sdo+qjpHVaPuZy7O2/Na5J6RL8B5RaYxph14SdpCEbnKLeb63FeDFHnc/m9w3kpgL+wypp14ej8tTi+Mu4CdwGXutGaJyIXAHlVd0cJy1u+xMa3gqd/jg9qwyH/jPB6KAiEgG/iHql7V1DrW77Exjub6PW6u54rf0UzlCVW9tbmdquo9wD3uts4C7mguYY0x3jT3nNZOecZ0Qs11N/NMe+1EVd8F3m2v7RnTlVl7WmNSjCWtMSnGSzXGU71MM8Ykh5cz7e88TjPGJEFzj3xOAb4B9BGRn8TMygZ8iQ7MGBNfc4980oEsd5nYjstLcGpFGWM6QHOPfN4D3hORuaq6NYkxGWOa4aVjt6CIPAUMjV1eVc9JVFDGmKZ5SdoXgdk4zeuqExuOMaYlXpI2qqqzEh6JMcYTL498XhGRm0Skv4j0rv0kPDJjTFxezrTXuMM7Y6YpMLz9wzHGtMRLb4zDkhGIMcYbL9UYM0Xkv9w7yIjI0W6vFMaYDuC1N8YqnNpRAPnAQy2tJCIhEflERFaKyBoReaANcRpjXF6S9khV/SUQAVDVCpwXcbWkEjhHVY8HxgGTRWTiQUdqjAG83Yiqcl8BogAiciROQjZLnc6nytyvAfdj7/4xpo28nGnvA94ABovIPGAJTreoLXK7XM3DeQfQYlVddtCRGmMAb3ePF4vIp8BEnGLxDFUt9LJxVa0GxolIT2CBiIxR1dWxy4jI9cD1AEcccURr4zemy/Hac8VAnOZ46cAZInJJa3aiqvtw+oiaHGfeU6qaq6q5ffp4enGBMV1ai2daEXkaOA5Yw4E3BSjwjxbW6wNEVHWfe038TeAXbQvXGOPlRtREVT32ILbdH3hGRHw4Z/QXVNXenGdMG3lJ2o9E5FhV/bw1G1bVz4DxBxeWMaYpXpL2GZzE3YXzqEdwnugcl9DIjDFxeUnap3HeybMKe/udMR3OS9JuU9WFCY/EGOOJl6RdJyJ/w3kDfF1NKFVt9u6xMSYxvCRtBk6yToqZ1uIjH2NMYnipEXVdMgIxxnjTXGfld6nqL5t6T21L76c1xiRGc2fate7Q3lNrTCfSXGflr7ij5ar6Yuw8Ebk8oVEZY5rkpcHAPR6nGWOSoLlr2vOBKcBAEXk8ZlY2EE10YMaY+Jq7pt2Bcz17MbAiZnopcHsigzLGNK25a9qVwEoR+ZuqRpIYkzGmGV4qV5wsIvcDQ9zlaxsMWGflxnQAL0n7Z5zi8ArsBVzGdDgvSVusqq+3dsMiMhh4FuiH0zroKVX9bWu3Y4ypz0vSviMiv8KpaxzbYODTFtaLAj9V1U9FpDuwQkQWt7YxvTGmPi9JO8Ed5sZMU6DZl0qr6k5gpzteKiJrcTqIs6Q1pg28NBg4u607EZGhOF3PNOr32LpQNaZ1vLyAq6+I/FlEXne/HysiP/C6AxHJAuYDt6lqScP51oWqMa3jpRrjXOBNYID7fQNwm5eNi0gAJ2HnWaN5Y9qHl6Q9TFVfwO0fSlWjeHj0IyKC87horao+2qYojTF1vCTtfhHJ4cALuCYCxR7WOxWnQ7hzRCTP/Uw5+FCNMeDt7vFPgIXAkSLyAdAHuKyllVT1fby9EtMY0wpe7h5/KiJnAiNxknC91UU2puM0WTwWkZNEpB/UXceeCDwM/D8R6Z2k+IwxDTR3TfsHoApARM4AHsGpllgMPJX40Iwx8TRXPPap6l53/AqcusPzgfnui6KNMR2guTOtT0Rqk/pc4F8x87zcwDLGJEBzyfcc8J6IFAIVwL8BROQovD3yMcYkQHM9VzwsIktw3jP7lqrW9n2cBtySjOCMMY01W8xV1Y/jTNuQuHCM6cS+XApLfw09j4Bv3g/dDuuQMLzUiDKmaytYD3+bDs9cBIUbYOXf4fe58OmzUJP8t79a0hrTlLICePUn8OQpsPUD+OYDcGse3PgBHH4sLLwF5k6BPWtb3lY7srvAxjQUqYCPn4R/PwaRcjjpB3Dm3QeKw31GwrWvQd48eOvnMPs0+MYtcMZdkJ6Z8PBS5kxbWLKfp95Zz5eF+zs6FHOoqqlxir6/y4UlD8KwM+DHy2DKrxpfv4rA+Kvg5uVw3BXw/mPw5ETY+HbCw5QDN4U7Xm5uri5fHv99X8uW/INxS69now7kq+BwAgPGcsSoXIaPnkBadt8kR2oOOV8uhbf+C3auhP7j4LyHYehp3tff8j68ertzzTv6f8HkR6B7v4MOR0RWqGpu3HmpkrTsWUfpx3Mp2bqSzK/X0atmb92sMn8vqnKOIXvo8fj7jYW+o6HPKAiEkhS5SVkFG2DxvbDhdegxGM69F8ZcBmmNC6El4Qh//Xgr/XuEmHr8QNLSGjRii1bCB4/D0l+BP+hsK/f7kOZrdVgdkrQi8jRwIbBHVcd4WafZpG2gpHAnn336ITs3LMdX8DlH6jZGynZC4jRAUklDco5yErjvaOg7xrkWSQtAdRVURxoMY8crm5he5awfyHA+/lDMMNP5kfBnxAzdjy/g7aDV1EBNtP5HG0yLVkG0wrnuilRANOxcd0XcYTR8YF6kov6ykQqQNMjMgczeTpEvMwcy3WG3w5zpoZ5O8e9gqTr7qiyBcAmEi6Gy2BmPVDjHwx90jpsv3Rn6gwem+YPgi/nuC7QtnnjKCuC9R2D5HEjvBqf/BCbc4Px7NVBdo7y4fDu/enM9RfurADjhiJ48OHUMYwb2aLztok3w2k9h8zsw4AS46DfQ//hWhddRSXsGUAY8m4ikjRWOVPPR5iIWr97BhrUrObz8C471bWdit12MlG10r/iq1dtsV+I7kNyS5iZgNWh1/YRsT41+XNwfFa2B8r1QXuQkedx1/ZARm9Q5B5I6vRtUljoJWJuUlbWJGfO9vf+eusQOQbA7BLMh1ANC7jCY7fzY1PseO97DWa+6qv5Nptzvw1kzm3zm+smXe3nglTWs2VFC7pBe3HvRsazfVcojr69jb3kVV044gjsmjaRnZnr9FVVh9Xx4Y6ZzrCfcCGf/HwhmefpzO6x47PbC+GqikzZWTY2yMn8fb32+m8Wf7+aLPWVkUc7kPl/zrcOL6ZERwJ+ejs8fxJ8eJJAeJJCeQSA9SHowRHrQGQaDIQIB99fel+782qcFoCbS+KwWe7ZrdBZscEYEJynqPj4nqWO/N5wfO/SlN07EQGbjs73Pw4OBqnIoL3T+U+0vcoZ1391h7HjF1zgdmIiTEEE3aWoTJO6wR/3v6ZlQHXVKM9GwU6SMht0ShPu9ujJmeoN50YoGPxrFB87m0YqW/2ZfupO4I6fAtx6Ew46Ou9hX+yr470VrefWznfTvEeKeKcdw0XH9EfeMX1wR4bHFG3j2oy30yAhw1+RRXJE7uHGRueJrePsBWDEHsgfC+b+EYy5sMcwulbQNbS4oY7GbwCu2fU1r/lx/mhAK+AgFfGSkp5ER8NEzM50+WUH6dA9yWFa6OwzWDXOy0gn6W38NkxKqo05iBLrFvebrcNGqmEQublA8d4dV+2HEZBh2etxNVFRVM/u9Tfxh6SZU4YYzj+R/nzmczPT4P4Jrd5Zw38tr+GTLXo4f1IMHpo5h3OCejRfc/gm8chvsWePcqLpsTrNF/k6dtA36PT5x69atCYtnf2WUssoo4Ug1FZFqKqqcYWWkpt73sPtxptUQjlYTrqqmvKqaveVVFJZVUlBaSWk4fhGwR0agXkLXJnWfrCA9MwNkZwToHvKTHQqQHQqQFfLja/gLbZJKVXnls508smgtO4rDXHhcf2aeP4pBvVp+7qqqvJy3g4cXraWwrJIrcgdz1+RR9O7WoMhcHXGK5qpwWvMdmnbqpI2ViDNtIoUj1RSWVVJYVkVBaaUzXlpJQVllXWLXziurbP4aLyvor0vk7iF/vcSO/e5PE/fHpcb9walu8INTU+9HqfYHp8KdFvClMbhXJkNyMjkiJ5MhvbtxRG/ne/8eIfy+TngGTbBV+cU88Moalm/9mtEDsrnvotGcPKz1nbOUhiP89u2NzPlwC1lBP3dMGsF3Jww5qB9kS9pOIByppqC0kuKKCCUVEUrCUUrCEUrDUUoq3GE4Qmk4QklFlNJKdxh2lq2uif/vFAqkOcV39xMM+MgIpJGRHvvd+YQCaVRGa9i2t5xtReVs/7qcSPWB7frThEG9MhjsJvGQ3t3qxo/onUm3YOMiYlW0pq4EUxp2hmWVkQPj4Sj7K6OUuuMVkWqC/gOXGxkBH6H0AzFmpPsO/D3ptXEfGM9057eHgtJKfv3mel5YsZ3emenced5ILs8d3OZSz4bdpdz38ho+2lzE6AHZPDh1DCcO6dWqbXTU3ePngLOAw4DdwH2q+ufm1jmUk7YtVJWKSDUlFVGqVcl0/yMH/WmNb3y0QnWNsqskzNai/WzfW87WonK2ugm9bW85xRX1++9zivrpVESqKQs7iVgVbbnCvIhTksgK+skI+KiMxpQGItWtus8A0D3op1+PEP16hOibHaJ/nGHvbul1N40aqorWMPfDL/ndki+oiFRz3alDueXco8kOeXw054Gq8tqqnTz06lp2lYS59IRBzDx/FH26Bz2tf2hUrjBJV1weYeve/WxzE3pbUTlF+6vICvroFvSTFfLT3U3GrFCgLjGzQv664n5tojb146Kq9ZM45r5CRVVNXWLXFvHLKqMUlFayqzjMrpIwu4rD7CkN07Agku5Lo2+PIP2y6yd0RrqPP/37S74s3M85ow7nZxccw5F9vD2GORj7K6P8/p0v+NO/NxPy+7j9WyO4+pQhLV6GWNKaQ1q0uobCsio3iSvYVRxmZ0mY3TGJvaskTDjilAqG9+nGzy88lrNHHp60GDcVlHH/wjX8e2MhJw/rzfPXT2yyJADNJ6218jEpz+9LqysuE+9xC84ZvbgiQmFZFUNyMgkk+YbbkX2yePb7J/Pmmt2EI9XNJmxLLGlNlyAi9MxMb1xzKckxTB5z8I0IanW9+/vGpDhLWmNSjCWtMSnGktaYFGNJa0yKsaQ1JsVY0hqTYixpjUkxlrTGpBhLWmNSjCWtMSkmoUkrIpNFZL2IfCEiMxO5L2O6ioQlrYj4gCeA84Fjge+IyLGJ2p8xXUUiz7QnA1+o6mZVrQL+DkxN4P6M6RISmbQDge0x3/PdacaYNkhke9p4rXwbdZMR24UqUCYi65vZ5mFAYTvElkidPcbOHh90/hiTEd+QpmYkMmnzgcEx3wcBOxoupKpPAU952aCILG+qC47OorPH2Nnjg84fY0fHl8ji8X+Ao0VkmIikA9OBhQncnzFdQsLOtKoaFZGbgTcBH/C0qq5J1P6M6SoS2keUqi4CFrXjJj0VoztYZ4+xs8cHnT/GDo2vU3WhaoxpmVVjNCbFdMqkban6o4gEReR5d/4y951ByYxvsIi8IyJrRWSNiMyIs8xZIlIsInnu594kx7hFRFa5+27UA7w4HneP4WcickISYxsZc1zyRKRERG5rsEzSj5+IPC0ie0Rkdcy03iKyWEQ2usO4L+URkWvcZTaKyDUJDVRVO9UH56bVJmA4kA6sBI5tsMxNwGx3fDrwfJJj7A+c4I53BzbEifEsnJePddRx3AIc1sz8KcDrOM/TJwLLOvDfexcwpKOPH3AGcAKwOmbaL4GZ7vhM4Bdx1usNbHaHvdzxXomKszOeab1Uf5wKPOOOvwScK23psr2VVHWnqn7qjpcCa0m92l5TgWfV8THQU0T6d0Ac5wKbVDVxLyb2SFWXAnsbTI79v/YM8O04q54HLFbVvar6NbAYmJyoODtj0nqp/li3jKpGgWIgJynRNeAWzccDy+LMPkVEVorI6yIyOqmBObXP3hKRFW6ts4Y6SzXT6cBzTczryONXq6+q7gTnxxqI9wKgpB7LzvhaEC/VHz1VkUw0EckC5gO3qWpJg9mf4hT5ykRkCvBP4Ogkhneqqu4QkcOBxSKyzj2T1OrwY+hWurkYuCfO7I4+fq2R1GPZGc+0Xqo/1i0jIn6gB42LNQklIgGchJ2nqv9oOF9VS1S1zB1fBARE5LBkxaeqO9zhHmABzmVHLE/VTBPsfOBTVd3dcEZHH78Yu2svG9zhnjjLJPVYdsak9VL9cSFQe4fuMuBf6t4RSAb3+vnPwFpVfbSJZfrVXmeLyMk4x7ooSfF1E5HutePAJGB1g8UWAle7d5EnAsW1xcAk+g5NFI078vg1EPt/7Rrg5TjLvAlMEpFe7t3lSe60xOiIO4Ye7uJNwbkjuwn4mTvtQeBidzwEvAh8AXwCDE9yfKfhFH8+A/LczxTgBuAGd5mbgTU4d78/Br6RxPiGu/td6cZQewxj4xOcTgo2AauA3CQfw0ycJOwRM61Djx/OD8hOIIJz9vwBzr2SJcBGd9jbXTYX+FPMut93/z9+AVyXyDitRpQxKaYzFo+NMc2wpDUmxVjSGpNiLGmNSTGWtMakGEvaQ4iIVDdoPdNuHcSLyNDY1i+m43TGaozm4FWo6riODsIklp1puwC3be0vROQT93OUO32IiCxx29MuEZEj3Ol9RWSBW1l/pYh8w92UT0T+6LYhfktEMtzlbxWRz93t/L2D/swuw5L20JLRoHh8Rcy8ElU9Gfg98Bt32u9xmucdB8wDHnenPw68p6rH47Qvre2Q72jgCVUdDewDLnWnzwTGu9u5IVF/nHFYjahDiIiUqWpWnOlbgHNUdbPb0GGXquaISCHQX1Uj7vSdqnqYiBQAg1S1MmYbQ3HajB7tfr8bCKjqQyLyBlCG0xLnn+pW9DeJYWfarkObGG9qmXgqY8arOXBP5AKceswnAivcllcmQSxpu44rYoYfueMf4rSiArgSeN8dXwLcCM7bD0Uku6mNikgaMFhV3wHuAnoCjc72pv3YL+KhJUNE8mK+v6GqtY99giKyDOeH+jvutFuBp0XkTqAAuM6dPgN4SkR+gHNGvRGn9Us8PuCvItIDp+XQY6q6r93+ItOIXdN2Ae41ba6qduaXWhmPrHhsTIqxM60xKcbOtMakGEtaY1KMJa0xKcaS1pgUY0lrTIqxpDUmxfx/d7SGMeaYqpsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 252x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 14s, sys: 13.8 s, total: 4min 28s\n",
      "Wall time: 48.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#torch.manual_seed(0) # SET SEED FOR TESTING\n",
    "W = torch.eye(nhidden,    nhidden,   dtype=torch.float64, requires_grad=True)\n",
    "U = torch.randn(nhidden,  nfeatures, dtype=torch.float64, requires_grad=True) # embed one-hot char vec\n",
    "V = torch.randn(nclasses, nhidden,   dtype=torch.float64, requires_grad=True) # take RNN output (h) and predict target\n",
    "\n",
    "optimizer = torch.optim.Adam([W,U,V], lr=0.005, weight_decay=0.0)\n",
    "\n",
    "history = []\n",
    "epochs = 12\n",
    "for epoch in range(1, epochs+1):\n",
    "#     print(f\"EPOCH {epoch}\")\n",
    "    epoch_training_loss = 0.0\n",
    "    epoch_training_accur = 0.0\n",
    "    total = 0\n",
    "    for p in range(0, n, batch_size):  # do one epoch\n",
    "        loss = 0\n",
    "        batch_X = X_train[p:p+batch_size]\n",
    "        batch_y = y_train[p:p+batch_size]\n",
    "        batch_X_onehot = onehot_matrix(batch_X, max_len, vocab)\n",
    "        H = torch.zeros(nhidden, batch_size, dtype=torch.float64, requires_grad=False)\n",
    "        for t in range(max_len):\n",
    "            x_step_t = batch_X_onehot[:,t].T # make it len(vocab) x batch_size\n",
    "            H = W.mm(H) + U.mm(x_step_t)\n",
    "            H = torch.tanh(H)\n",
    "        o = V.mm(H)\n",
    "        o = o.T # make it batch_size x nclasses\n",
    "        o = softmax(o)\n",
    "        loss = cross_entropy(o, batch_y)\n",
    "#         print(loss.item())\n",
    "        correct = torch.argmax(o, dim=1)==batch_y\n",
    "        epoch_training_accur += torch.sum(correct)\n",
    "        total += len(batch_y)\n",
    "\n",
    "        # update matrices based upon loss computed from a batch\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # autograd computes U.grad, M.grad, ...\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_training_loss += loss.detach().item()\n",
    "\n",
    "    epoch_training_loss /= nbatches\n",
    "    epoch_training_accur /= n\n",
    "#     print(f\"Epoch {epoch:3d} training loss {epoch_training_loss:7.4f} accur {epoch_training_accur:7.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        o = forward(X_train, max_len, vocab)#, apply_softmax=False)\n",
    "        train_loss = cross_entropy(o, y_train).item()\n",
    "        correct = torch.argmax(o, dim=1).detach()==torch.tensor(y_train)\n",
    "        train_accur = torch.sum(correct) / float(len(X_train))\n",
    "\n",
    "        o = forward(X_valid, max_len, vocab)\n",
    "        valid_loss = cross_entropy(o, y_valid).item()\n",
    "        correct = torch.argmax(o, dim=1).detach()==torch.tensor(y_valid)\n",
    "        valid_accur = torch.sum(correct) / float(len(X_valid))\n",
    "\n",
    "        history.append((train_loss, valid_loss))\n",
    "        print(f\"Epoch: {epoch:3d} accum loss {epoch_training_loss:7.4f} accur {epoch_training_accur:4.3f} | train loss {train_loss:7.4f} accur {train_accur:4.3f} | valid loss {valid_loss:7.4f} accur {valid_accur:4.3f}\")\n",
    "\n",
    "history = torch.tensor(history)\n",
    "plot_history(history, yrange=(0,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing on 80% training from full data set:\n",
    "\n",
    "```\n",
    "CPU times: user 40.3 s, sys: 749 ms, total: 41 s\n",
    "Wall time: 41 s\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
